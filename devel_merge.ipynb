{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from pandas import HDFStore,DataFrame\n",
    "# define the current path (notebooks in lab_utils)\n",
    "currpath = os.getcwd()\n",
    "labutilspath = str(Path(currpath).parents[1])\n",
    "sys.path.append(labutilspath)\n",
    "\n",
    "# import the autoscan routines\n",
    "from autoscan import autoscan\n",
    "\n",
    "# define paths\n",
    "basepath = '/media/damiansa/data/lab/data/characterization/'\n",
    "\n",
    "# set the asdatapath accordingly (where is the atuoscan data inside basedatapath?)\n",
    "asdatapath   = 'autoscan'\n",
    "\n",
    "#sample selection and instance\n",
    "tag = 'sh_001'\n",
    "instance = 'before'\n",
    "\n",
    "# set datapath\n",
    "datapath = os.path.join(basepath, asdatapath)\n",
    "\n",
    "pp = autoscan.postprocess(labutilspath=labutilspath)\n",
    "pp.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the summary dataframe with all measurement\n",
    "summary = pd.read_csv(os.path.join(datapath,'summary.csv'))\n",
    "# remove ztop and zbottom\n",
    "t = summary['relroot'].apply(lambda x: len(re.findall(r'ztop|zbottom', x))==0)\n",
    "summary = summary.loc[t,:].copy()\n",
    "\n",
    "sample_info = pp._subset_info(summary, instance = instance, sample_tag = tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dfs = []\n",
    "# for s in sample_info.itertuples(index=False):\n",
    "#     relpath = getattr(s, 'relroot')\n",
    "#     probe   = getattr(s, 'probe')\n",
    "#     data = pp.read_data(os.path.join(datapath, relpath), probe = probe)\n",
    "#     h = pp.probe_settings[probe]['h']\n",
    "#     data = data.iloc[:,:h]\n",
    "#     if probe == 'vel':\n",
    "#         for x in data.angle.unique():\n",
    "#             dfs.append(data.loc[data['angle'] == x,:].copy().drop(columns = 'angle'))\n",
    "#     else:\n",
    "#         dfs.append(data)\n",
    "\n",
    "# df_merged = reduce(lambda left,right: pd.merge(left,right,on=['x','y'], how ='inner'), dfs)\n",
    "# df_merged = pp._enforce_float(df_merged)\n",
    "# df_merged['tag'] = tag\n",
    "# df_merged['instance'] = instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "fullset = []\n",
    "problem = {}\n",
    "problem_merged = []\n",
    "ns = 0\n",
    "for instace in ['before', 'after']:\n",
    "    for tag in summary['sample_tag'].unique():\n",
    "        s1 = pp._subset_info(summary, instance = instance, sample_tag = tag)\n",
    "        if len(s1)>0:\n",
    "            family = s1['sample_family'].unique()[0]\n",
    "            code   = s1['sample_code'].unique()[0]\n",
    "            for subtag in s1['subsample_tag'].unique():\n",
    "                s2 = pp._subset_info(s1, subsample_tag = subtag, sample_code = code, sample_family = family)\n",
    "                for kside, side in enumerate(s2['side'].unique()):\n",
    "                    s3 = pp._subset_info(s2, side = side)\n",
    "                    probes  = s3['probe'].unique()\n",
    "                    nprobes = len(probes)\n",
    "                    if nprobes>1 and ('ftir' in probes):\n",
    "                        dfs = []\n",
    "                        for meas in s3.itertuples(index=False):\n",
    "                            relpath  = getattr(meas, 'relroot')\n",
    "                            filepath = os.path.join(datapath, relpath)\n",
    "                            probe    = getattr(meas, 'probe')\n",
    "                            \n",
    "                            data     = pp.read_data(filepath, probe = probe)\n",
    "                            \n",
    "                            h    = pp.probe_settings[probe]['h']\n",
    "                            data = data.iloc[:,:h]\n",
    "\n",
    "                            if probe == 'vel':\n",
    "                                for k,x in enumerate(data.angle.unique()):\n",
    "                                    colnames = ['vp_'+str(k), 'vs_'+str(k)]\n",
    "                                    data_temp = data.loc[data['angle'] == x,:].copy().drop(columns = 'angle')\n",
    "                                    data_temp.columns = ['x','y'] + colnames\n",
    "                                    dfs.append(data_temp)\n",
    "                                    del data_temp\n",
    "                            else:\n",
    "                                dfs.append(data)\n",
    "\n",
    "                        df_merged = reduce(lambda left,right: pd.merge(left,right,on=['x','y'], how ='inner'), dfs)\n",
    "                        df_merged = pp._enforce_float(df_merged)\n",
    "\n",
    "                        if df_merged.shape[0]>0 and len(df_merged.iloc[:,1754:].columns.values)>1:\n",
    "                            ns += df_merged.shape[0]\n",
    "                            df_merged['family']   = family\n",
    "                            df_merged['code']     = code\n",
    "                            df_merged['tag']      = tag\n",
    "                            df_merged['subtag']   = subtag\n",
    "                            df_merged['instance'] = instance\n",
    "                            df_merged['side']     = kside\n",
    "                            \n",
    "                            if save:\n",
    "                                outpath = pp._set_outpath(datapath,tag)\n",
    "                                outname = pp._set_outfilename(tag, subtag, side, instance, 'fullset')\n",
    "                                pp.save_data(df_merged, savepath = outpath,  savename = outname)\n",
    "                        \n",
    "                            fullset.append(df_merged.iloc[:,2:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns\n",
    "compiledinfocols = ['family', 'code', 'tag', 'subtag', 'side', 'instance']\n",
    "compileddatacols = pp.probe_settings['ftir']['names'][2:] + ['perm', 'vp_0', 'vs_0', 'vp_1', 'vs_1', 'e_star']  \n",
    "compiledfullcols = compiledinfocols + compileddatacols\n",
    "# fullset[1].columns[1750:]\n",
    "dfc = pd.concat(fullset, sort = False, axis = 0, ignore_index=True, join = 'outer')\n",
    "dfc = dfc.loc[:,compiledfullcols].copy()\n",
    "dfc.loc[:,compileddatacols] = dfc.loc[:,compileddatacols].apply(pd.to_numeric,errors='coerce').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefile = os.path.join(datapath, 'as_dataset.h5')\n",
    "dfc.to_hdf(savefile, key = 'data', mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdish as dp\n",
    "dp.io.save(os.path.join(datapath, 'data.h5'), dfc.loc[:, compileddatacols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.io.save(os.path.join(datapath, 'descriptions.h5'), dfc.loc[:, compiledinfocols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/damiansa/sandbox/opt/anaconda/lib/python3.7/site-packages/deepdish/io/hdf5io.py:246: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  elif _pandas and isinstance(level, (pd.DataFrame, pd.Series, pd.Panel)):\n"
    }
   ],
   "source": [
    "dp.io.save(os.path.join(datapath, 'compiled.h5'), dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}