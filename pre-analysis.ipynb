{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, io, pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import hiplot as hip\n",
    "import klib\n",
    "import seaborn as sns\n",
    "\n",
    "buffer = io.StringIO()\n",
    "mix = pd.IndexSlice\n",
    "\n",
    "# define the current path (notebooks in lab_utils)\n",
    "labutilspath = str(pathlib.Path(os.getcwd()).parents[1])\n",
    "sys.path.append(labutilspath)\n",
    "\n",
    "# import the autoscan routines\n",
    "from autoscan import autoscan\n",
    "\n",
    "pp = autoscan.basics()\n",
    "\n",
    "def probeix(df, vmin, vmax):\n",
    "    a = (df >= vmin)\n",
    "    b = (df <= vmax)\n",
    "    if not isinstance(df, pd.Series):\n",
    "        a = a.all(axis = 1)\n",
    "        b = b.all(axis = 1)\n",
    "    \n",
    "    return np.logical_and(a, b)\n",
    "\n",
    "def test(x, th = 0.5, vmin = 0, vmax = 1e6):\n",
    "    s = probeix(x, vmin = vmin, vmax = vmax)\n",
    "    test = s.sum() / len(s) >= th\n",
    "#     ntrue  = s.sum()\n",
    "#     nfalse = (s == False).sum()\n",
    "    return test\n",
    "\n",
    "def get_wrong_measurements(df, probe = None, desc = None, th = 0.5, vmin = 0, vmax = 1e6):\n",
    "#     desc2 = desc.sort_index()\n",
    "    if desc is None:\n",
    "        levels = df.index.names\n",
    "    else:\n",
    "        levels = desc.index.names\n",
    "    \n",
    "    ix = df.groupby(level = levels).apply(test, vmin = vmin, vmax = vmax)\n",
    "    \n",
    "    if not np.logical_or(probe is None, desc is None):\n",
    "        out = desc.loc[ix[ix == False].index].query(\"probe == '%s'\" %(probe))\n",
    "        out = (out, ix)\n",
    "    else:\n",
    "        out = ix\n",
    "    \n",
    "    return out\n",
    "\n",
    "def pprint(msg, msg_title = '', msg_decorator = '#', len_decorator = 40):\n",
    "    nhead = len_decorator - len(msg_title) - 2\n",
    "    if nhead <= 0:\n",
    "        nhead = 1\n",
    "        nfoot = len(msg_title) + 4\n",
    "    else:\n",
    "        nfoot = len_decorator\n",
    "    \n",
    "    top_decorator = msg_decorator * (nhead // 2) \n",
    "    print(top_decorator + ' ' + msg_title  +  ' ' + top_decorator, \n",
    "          msg, nfoot * '#' + '\\n',\n",
    "          sep = '\\n')\n",
    "    return\n",
    "\n",
    "def dfinfo(df, header = 'info'):\n",
    "    with io.StringIO() as buffer:\n",
    "        df.info(buf = buffer)\n",
    "        pprint(buffer.getvalue(), msg_title = header)\n",
    "\n",
    "def interp_on_nans(d, debug = False, extrap = np.mean):\n",
    "    mask = d.iloc[:, -1].isna().values == False\n",
    "    if not mask.all():\n",
    "    #     x, y, v = d.values.T\n",
    "        c = (d.iloc[:, :2] == d.iloc[0, :2]).all() == False\n",
    "        c = d.columns[np.append(c, True)]\n",
    "        mean = extrap(d.iloc[mask, -1])\n",
    "        if len(c) == 2:\n",
    "            v = np.interp(d.loc[mask == False, c[0]], d.loc[mask, c[0]], d.iloc[mask, -1], left = mean, right = mean)\n",
    "        elif len(c) == 3:\n",
    "            try:\n",
    "                v = interpolate.griddata(d.loc[mask, coords], d.iloc[mask, -1], d.loc[mask == False, coords], fill_value = mean)\n",
    "            except:\n",
    "                if debug: print('interp failed for: ', g)\n",
    "                v = mean\n",
    "        \n",
    "        d.iloc[mask == False, -1] = v\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/urlab/sandbox/data/characterization/autoscan/autoscan.h5'\n",
    "savepath = '/home/urlab/Documents/'\n",
    "\n",
    "# load the data\n",
    "df = pd.read_hdf(datapath, key = 'data')\n",
    "desc = pd.read_hdf(datapath, key = 'description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing\n",
    "\n",
    "1. pre-clean the dataset\n",
    " - remove duplicated rows\n",
    " - enforce correct dtypes \n",
    " - reduce memory overhead\n",
    " - do not remove missing values\n",
    "1. create a set of variables to summarize the ftir data\n",
    "1. visualize\n",
    " - relations in the dataset suing `hip.Experiment.from_dataframe(df).display()`\n",
    " - distribution of missing values  `klib.missingval_plot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-cleaning\n",
    "first cleaning of the data before inputation.\n",
    "overwrite the df since there is no need to refer to it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the record print the information of the original dataframe\n",
    "dfinfo(df, 'raw data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-clean, do not remove missing values\n",
    "df = klib.data_cleaning(df, drop_threshold_rows = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the information of the cleaned dataframe\n",
    "dfinfo(df, 'raw data cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ftir stats (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'l_mean'] = df.iloc[:, 2:1754].mean(axis = 1)\n",
    "df.loc[:, 'l_std'] = df.iloc[:, 2:1754].std(axis = 1)\n",
    "df.loc[:, 'l_median'] = df.iloc[:, 2:1754].median(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize\n",
    "### feature flow with hip\n",
    "in this step the features of the ftir can be summarized within the feature `l_mean`\n",
    "\n",
    "visualize the flow of values in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcols = df.columns[-9:-2]\n",
    "hip.Experiment.from_dataframe(df.reset_index().loc[:, np.append(['family', 'code'],pcols)]).display();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing values wtih klib\n",
    "use `klib` to visualize the missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klib.missingval_plot(df.loc[:,pcols]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing values & outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_ix = df.index.droplevel(6).drop_duplicates()\n",
    "ds = desc.loc[desc_ix].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = 'vel'\n",
    "\n",
    "# get the columns for velocity\n",
    "vcols = pp.probe_settings[probe]['col'][:4]\n",
    "\n",
    "# get min and max expected for measurement\n",
    "vmin, vmax = pp.probe_settings[probe]['limits']\n",
    "\n",
    "# pcols = df.columns[df.columns.str.startswith(probe[0])]\n",
    "ncols = len(vcols[2:])\n",
    "\n",
    "# checkout features\n",
    "dv = df.loc[:, vcols[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfinfo(dv, 'info of raw')\n",
    "## firtst check how they are distributed\n",
    "pprint(dv.describe().apply(np.round, decimals = 2), 'raw data')\n",
    "\n",
    "## identify offending values and those to keep\n",
    "vix = probeix(dv.iloc[:, -1], vmin = vmin, vmax = vmax)\n",
    "\n",
    "## set nan for all incorrect values\n",
    "dv.iloc[vix.values == False, -1] = np.nan\n",
    "pprint(dv.describe().apply(np.round, decimals = 2), 'correct measurements')\n",
    "\n",
    "# get the labels that have problem\n",
    "out, ix = get_wrong_measurements(dv.iloc[:, -1], probe = probe, desc = ds, vmin = vmin, vmax = vmax)\n",
    "ixd = ixdp = dv.loc[ix == True].index\n",
    "ixdn = dv.loc[ix == False].index\n",
    "\n",
    "pprint(dv.loc[ix, :].describe().apply(np.round, decimals = 2), 'only approved samples')\n",
    "\n",
    "# the ix returned from `get_wrong_measurements` keeps only samples where  more than a threshold percent (`th`) of values are correct.\n",
    "# samples that don't meet this criteria are lost. This is different than probeix, which only asserts if the values are within a range independently of their sample. \n",
    "\n",
    "# ix can be used to do basic data inputation on the sample, for example by filling it with the mean\n",
    "dp = dv.copy()\n",
    "dp.loc[ix, :] = dv.loc[ix, :].groupby(level = ds.index.names).apply(interp_on_nans)\n",
    "pprint(dp.describe().apply(np.round, decimals = 2), 'correct & interp data (all samples)')\n",
    "\n",
    "dp = dp.groupby(level = ds.index.names).apply(lambda x: x.fillna(x.mean()))\n",
    "pprint(dp.describe().apply(np.round, decimals = 2), 'correct & interp data (all samples, fillna)')\n",
    "# dv.iloc[[ix == True], -1] = dv.iloc[ix == True, -1].groupby(level = desc.index.names).apply(lambda x: x.fillna(x.mean()))\n",
    "# # ixs = dv.dropna().index\n",
    "\n",
    "# # set all the samples that did not meet the criteria to nan\n",
    "# # dv2 = dv.copy()\n",
    "# # dv2.loc[mix[ix == False, :]] = np.nan\n",
    "# # dfinfo(dv2, 'info of mix')\n",
    "\n",
    "pprint('index\\t len\\t +\\t -\\t \\nvix\\t %d \\nix\\t %d \\t %d \\t %d \\nixd\\t %d \\t %d \\t %d' \n",
    "       % tuple([len(x) for x in [vix, ix, ix[ix == True], ix[ix == False], ixd, ixdp, ixdn]]),\n",
    "      'index lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp.loc[:, vcols[-1]].dropna().groupby('code').hist();#agg({vcols[-1] : ['mean', 'std', 'median','count']})\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "dp.loc[ix, vcols[-1]].groupby(level = 'code').plot(kind = 'kde', y = vcols[-1], grid = True, ax = ax, label = 'code');\n",
    "#hist(by = 'code', column = vcols[-1], sharex = True, sharey = True);\n",
    "# ds = pd.DataFrame(columns = ['step', vcols[-1]])\n",
    "# [d.loc[k, vcols[-1]].reset_index(drop = True) for k in [[True]*dv.shape[0], vix, ix] for d in [dv, dp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.append(velcols, [permcol, hammcol])\n",
    "v = np.append(['family','code'], v)\n",
    "df2 = df.loc[ix, :].reset_index(drop = False)\n",
    "# df2.loc[ixd.values, v.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaborn.pydata.org/generated/seaborn.violinplot.html\n",
    "# https://levelup.gitconnected.com/scikit-learn-python-6-useful-tricks-for-data-scientists-1a0a502a6aa3\n",
    "# https://towardsdatascience.com/speed-up-your-data-cleaning-and-preprocessing-with-klib-97191d320f80\n",
    "# https://pythondata.com/dask-large-csv-python/\n",
    "# https://github.com/wiseio/paratext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# perm = df.loc[:, [permcol]]\n",
    "# pprint(perm.describe())\n",
    "# permidx = probeix(perm, vmin = 0, vmax = np.inf)\n",
    "# ## check the data makes sense\n",
    "# pprint(perm.loc[permidx, :].describe())\n",
    "\n",
    "# ## print the labels that have problem\n",
    "# out, _  = get_wrong_measurements(perm, 'perm', desc)\n",
    "# out.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klib.dist_plot(df2.loc[:, v[[1,2,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp.iloc[:, -1].groupby('code').apply(klib.dist_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove offending values and keep just good measurements\n",
    "idx = np.logical_and(velidx.values, permidx.values)\n",
    "dc = df.loc[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative, fill the values with means or nans\n",
    "vels.loc[velidx == False, :] = np.nan\n",
    "desc.loc[vels.index[velidx == False].droplevel(6).drop_duplicates()].query(\"probe == 'vel'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = velidx[velidx == False].index.droplevel(6).drop_duplicates()\n",
    "# for t in x:\n",
    "#     vels.loc[mix[[*t], :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = vels.groupby(level = velidx.index.names[:-1], sort = False).apply(np.mean)\n",
    "\n",
    "# for dd in means.index:\n",
    "#     pass\n",
    "# dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix = pd.IndexSlice\n",
    "# mix[[*ix], :]\n",
    "# vels.loc(axis = 0)[mix[[dd], :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myquery(x, vels, velmin = 0.5e3, velmax = v):\n",
    "#     v = vels.copy()\n",
    "#     s = v.loc[x, :].shape[0]\n",
    "#     idx = np.logical_and((v >= velmin).all(axis = 1), (v <= velmax).all(axis = 1))\n",
    "#     skeep = np.sum(idx)\n",
    "#     sdrop = np.sum(idx == False)\n",
    "#     return s, skeep, sdrop\n",
    "\n",
    "# for a, b in idx.loc[revise_idx, :].groupby(level = idx.index.names[2:-1]):\n",
    "#     pass#.describe()\n",
    "\n",
    "# x = revise_idx.droplevel(6).drop_duplicates()\n",
    "# idx.loc[slice(x, :), :]\n",
    "\n",
    "# idx.loc[revise_idx, :]\n",
    "# idx.groupby(level = idx.index.names[2:-1]).describe()\n",
    "# .loc[:, 'r'] = 'review'\n",
    "\n",
    "# mix = pd.IndexSlice\n",
    "# pd.concat((idx.loc[mix[[*t], :]] for t in x)) \n",
    "\n",
    "# mix = pd.IndexSlice\n",
    "# pd.concat((idx.loc[mix[[*t], :]] for t in x)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
