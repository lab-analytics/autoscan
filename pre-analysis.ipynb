{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, io, pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import hiplot as hip\n",
    "import klib\n",
    "import seaborn as sns\n",
    "from dask import dataframe as dd\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "buffer = io.StringIO()\n",
    "mix = pd.IndexSlice\n",
    "\n",
    "# define the current path (notebooks in lab_utils)\n",
    "labutilspath = str(pathlib.Path(os.getcwd()).parents[1])\n",
    "sys.path.append(labutilspath)\n",
    "\n",
    "# import the autoscan routines\n",
    "from autoscan import autoscan\n",
    "\n",
    "pp = autoscan.basics()\n",
    "\n",
    "def probeix(df, vmin, vmax):\n",
    "    a = (df >= vmin)\n",
    "    b = (df <= vmax)\n",
    "    if not isinstance(df, pd.Series):\n",
    "        a = a.all(axis = 1)\n",
    "        b = b.all(axis = 1)\n",
    "    \n",
    "    return np.logical_and(a, b)\n",
    "\n",
    "def test(x, th = 0.5, vmin = 0, vmax = 1e6):\n",
    "    s = probeix(x, vmin = vmin, vmax = vmax)\n",
    "    test = s.sum() / len(s) >= th\n",
    "#     ntrue  = s.sum()\n",
    "#     nfalse = (s == False).sum()\n",
    "    return test\n",
    "\n",
    "def get_wrong_measurements(df, probe = None, desc = None, th = 0.5, vmin = 0, vmax = 1e6):\n",
    "#     desc2 = desc.sort_index()\n",
    "    if desc is None:\n",
    "        levels = df.index.names\n",
    "    else:\n",
    "        levels = desc.index.names\n",
    "    \n",
    "    ix = df.groupby(level = levels).apply(test, vmin = vmin, vmax = vmax)\n",
    "    \n",
    "    if not np.logical_or(probe is None, desc is None):\n",
    "        out = desc.loc[ix[ix == False].index].query(\"probe == '%s'\" %(probe))\n",
    "        out = (out, ix)\n",
    "    else:\n",
    "        out = ix\n",
    "    \n",
    "    return out\n",
    "\n",
    "def pprint(msg, msg_title = '', msg_decorator = '#', len_decorator = 40):\n",
    "    nhead = len_decorator - len(msg_title) - 2\n",
    "    if nhead <= 0:\n",
    "        nhead = 1\n",
    "        nfoot = len(msg_title) + 4\n",
    "    else:\n",
    "        nfoot = len_decorator\n",
    "    \n",
    "    top_decorator = msg_decorator * (nhead // 2) \n",
    "    print(top_decorator + ' ' + msg_title  +  ' ' + top_decorator, \n",
    "          msg, nfoot * '#' + '\\n',\n",
    "          sep = '\\n')\n",
    "    return\n",
    "\n",
    "def dfinfo(df, header = 'info'):\n",
    "    with io.StringIO() as buffer:\n",
    "        df.info(buf = buffer)\n",
    "        pprint(buffer.getvalue(), msg_title = header)\n",
    "\n",
    "def interp_on_nans(d, debug = False, extrap = np.mean, coords = ['x', 'y']):\n",
    "    mask = d.iloc[:, -1].isna().values == False\n",
    "    if not mask.all():\n",
    "    #     x, y, v = d.values.T\n",
    "        c = (d.iloc[:, :2] == d.iloc[0, :2]).all() == False\n",
    "        c = d.columns[np.append(c, True)]\n",
    "        mean = extrap(d.iloc[mask, -1])\n",
    "        if len(c) == 2:\n",
    "            v = np.interp(d.loc[mask == False, c[0]], d.loc[mask, c[0]], d.iloc[mask, -1], left = mean, right = mean)\n",
    "        elif len(c) == 3:\n",
    "            try:\n",
    "                v = interpolate.griddata(d.loc[mask, coords], d.iloc[mask, -1], d.loc[mask == False, coords], fill_value = mean)\n",
    "            except:\n",
    "                if debug: print('interp failed for: ', g)\n",
    "                v = mean\n",
    "        \n",
    "        d.iloc[mask == False, -1] = v\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client, LocalCluster\n",
    "# client = Client()\n",
    "# cluster = LocalCluster()\n",
    "# client = Client(cluster)\n",
    "# cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n"
     ]
    }
   ],
   "source": [
    "# datapath = '/home/urlab/sandbox/data/characterization/autoscan/autoscan.h5'\n",
    "datapath = '/home/urlab/sandbox/data/characterization/autoscan/autoscan_parallel.h5'\n",
    "savepath = '/home/urlab/Documents/'\n",
    "\n",
    "# load the data\n",
    "da = dd.read_hdf(datapath, '/data')\n",
    "desc = dd.read_hdf(datapath, '/description').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ftir_row_stats(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            l_mean = lambda df: df.iloc[:, 2:1754].mean(axis = 1),          \n",
    "            l_std = lambda df: df.iloc[:, 2:1754].std(axis = 1),\n",
    "            # l_median = lambda df: np.median(df.iloc[:, 2:1754], axis = 1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def clean_dataframe(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .where(df >= 0, np.nan)\n",
    "        .astype(np.float32)\n",
    "    )\n",
    "\n",
    "def enforce_limits(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    for k, p in pp.probe_settings.items():\n",
    "        v = p['col'][2:]\n",
    "        vmin, vmax = p['limits']\n",
    "        df[v] = df[v].where(((df[v] >= vmin) & (df[v] <= vmax)), np.nan)\n",
    "    return df\n",
    "\n",
    "def compute_final_dataframe(df: dd.DataFrame, workers = 20) -> pd.DataFrame:\n",
    "    \"\"\"Execute dask task graph and compute final results\"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .compute(num_workers = 20)\n",
    "    )\n",
    "\n",
    "def hip_visualize(df, pcols = None):\n",
    "    dp = df.reset_index().loc[:, np.append(['family', 'code'], pcols)]\n",
    "    s = hip.Experiment.from_dataframe(dp).display();\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = clean_dataframe(da)\n",
    "db = enforce_limits(db)\n",
    "db = ftir_row_stats(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  7.2s\n",
      "[########################################] | 100% Completed |  7.3s\n",
      "CPU times: user 27.5 s, sys: 2.32 s, total: 29.8 s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df = db.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip_visualize(df, df.columns[-9:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing\n",
    "\n",
    "1. pre-clean the dataset\n",
    " - remove duplicated rows\n",
    " - enforce correct dtypes \n",
    " - reduce memory overhead\n",
    " - do not remove missing values\n",
    "1. create a set of variables to summarize the ftir data\n",
    "1. visualize\n",
    " - relations in the dataset suing `hip.Experiment.from_dataframe(df).display()`\n",
    " - distribution of missing values  `klib.missingval_plot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-cleaning\n",
    "first cleaning of the data before inputation.\n",
    "overwrite the df since there is no need to refer to it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the record print the information of the original dataframe\n",
    "dfinfo(df, 'raw data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-clean, do not remove missing values\n",
    "df = klib.data_cleaning(df, drop_threshold_rows = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the information of the cleaned dataframe\n",
    "dfinfo(df, 'raw data cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ftir stats (basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize\n",
    "### feature flow with hip\n",
    "in this step the features of the ftir can be summarized within the feature `l_mean`\n",
    "\n",
    "visualize the flow of values in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip_visualize(df, df.columns[-9:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing values wtih klib\n",
    "use `klib` to visualize the missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klib.missingval_plot(df.loc[:,pcols]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing values & outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_ix = df.index.droplevel(6).drop_duplicates()\n",
    "ds = desc.loc[desc_ix].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = 'vel'\n",
    "\n",
    "# get the columns for velocity\n",
    "vcols = pp.probe_settings[probe]['col'][:4]\n",
    "\n",
    "# get min and max expected for measurement\n",
    "vmin, vmax = pp.probe_settings[probe]['limits']\n",
    "\n",
    "# pcols = df.columns[df.columns.str.startswith(probe[0])]\n",
    "ncols = len(vcols[2:])\n",
    "\n",
    "# checkout features\n",
    "dv = df.loc[:, vcols[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfinfo(dv, 'info of raw')\n",
    "## firtst check how they are distributed\n",
    "pprint(dv.describe().apply(np.round, decimals = 2), 'raw data')\n",
    "\n",
    "## identify offending values and those to keep\n",
    "vix = probeix(dv.iloc[:, -1], vmin = vmin, vmax = vmax)\n",
    "\n",
    "## set nan for all incorrect values\n",
    "dv.iloc[vix.values == False, -1] = np.nan\n",
    "pprint(dv.describe().apply(np.round, decimals = 2), 'correct measurements')\n",
    "\n",
    "# get the labels that have problem\n",
    "out, ix = get_wrong_measurements(dv.iloc[:, -1], probe = probe, desc = ds, vmin = vmin, vmax = vmax)\n",
    "ixd = ixdp = dv.loc[ix == True].index\n",
    "ixdn = dv.loc[ix == False].index\n",
    "\n",
    "pprint(dv.loc[ix, :].describe().apply(np.round, decimals = 2), 'only approved samples')\n",
    "\n",
    "# the ix returned from `get_wrong_measurements` keeps only samples where  more than a threshold percent (`th`) of values are correct.\n",
    "# samples that don't meet this criteria are lost. This is different than probeix, which only asserts if the values are within a range independently of their sample. \n",
    "\n",
    "# ix can be used to do basic data inputation on the sample, for example by filling it with the mean\n",
    "dp = dv.copy()\n",
    "dp.loc[ix, :] = dv.loc[ix, :].groupby(level = ds.index.names).apply(interp_on_nans)\n",
    "pprint(dp.describe().apply(np.round, decimals = 2), 'correct & interp data (all samples)')\n",
    "\n",
    "dp = dp.groupby(level = ds.index.names).apply(lambda x: x.fillna(x.mean()))\n",
    "pprint(dp.describe().apply(np.round, decimals = 2), 'correct & interp data (all samples, fillna)')\n",
    "# dv.iloc[[ix == True], -1] = dv.iloc[ix == True, -1].groupby(level = desc.index.names).apply(lambda x: x.fillna(x.mean()))\n",
    "# # ixs = dv.dropna().index\n",
    "\n",
    "# # set all the samples that did not meet the criteria to nan\n",
    "# # dv2 = dv.copy()\n",
    "# # dv2.loc[mix[ix == False, :]] = np.nan\n",
    "# # dfinfo(dv2, 'info of mix')\n",
    "\n",
    "pprint('index\\t len\\t +\\t -\\t \\nvix\\t %d \\nix\\t %d \\t %d \\t %d \\nixd\\t %d \\t %d \\t %d' \n",
    "       % tuple([len(x) for x in [vix, ix, ix[ix == True], ix[ix == False], ixd, ixdp, ixdn]]),\n",
    "      'index lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp.loc[:, vcols[-1]].dropna().groupby('code').hist();#agg({vcols[-1] : ['mean', 'std', 'median','count']})\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "dp.loc[ix, vcols[-1]].groupby(level = 'code').plot(kind = 'kde', y = vcols[-1], grid = True, ax = ax, label = 'code');\n",
    "#hist(by = 'code', column = vcols[-1], sharex = True, sharey = True);\n",
    "# ds = pd.DataFrame(columns = ['step', vcols[-1]])\n",
    "# [d.loc[k, vcols[-1]].reset_index(drop = True) for k in [[True]*dv.shape[0], vix, ix] for d in [dv, dp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.append(velcols, [permcol, hammcol])\n",
    "v = np.append(['family','code'], v)\n",
    "df2 = df.loc[ix, :].reset_index(drop = False)\n",
    "# df2.loc[ixd.values, v.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaborn.pydata.org/generated/seaborn.violinplot.html\n",
    "# https://levelup.gitconnected.com/scikit-learn-python-6-useful-tricks-for-data-scientists-1a0a502a6aa3\n",
    "# https://towardsdatascience.com/speed-up-your-data-cleaning-and-preprocessing-with-klib-97191d320f80\n",
    "# https://pythondata.com/dask-large-csv-python/\n",
    "# https://github.com/wiseio/paratext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# perm = df.loc[:, [permcol]]\n",
    "# pprint(perm.describe())\n",
    "# permidx = probeix(perm, vmin = 0, vmax = np.inf)\n",
    "# ## check the data makes sense\n",
    "# pprint(perm.loc[permidx, :].describe())\n",
    "\n",
    "# ## print the labels that have problem\n",
    "# out, _  = get_wrong_measurements(perm, 'perm', desc)\n",
    "# out.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klib.dist_plot(df2.loc[:, v[[1,2,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp.iloc[:, -1].groupby('code').apply(klib.dist_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove offending values and keep just good measurements\n",
    "idx = np.logical_and(velidx.values, permidx.values)\n",
    "dc = df.loc[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative, fill the values with means or nans\n",
    "vels.loc[velidx == False, :] = np.nan\n",
    "desc.loc[vels.index[velidx == False].droplevel(6).drop_duplicates()].query(\"probe == 'vel'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = velidx[velidx == False].index.droplevel(6).drop_duplicates()\n",
    "# for t in x:\n",
    "#     vels.loc[mix[[*t], :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = vels.groupby(level = velidx.index.names[:-1], sort = False).apply(np.mean)\n",
    "\n",
    "# for dd in means.index:\n",
    "#     pass\n",
    "# dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix = pd.IndexSlice\n",
    "# mix[[*ix], :]\n",
    "# vels.loc(axis = 0)[mix[[dd], :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myquery(x, vels, velmin = 0.5e3, velmax = v):\n",
    "#     v = vels.copy()\n",
    "#     s = v.loc[x, :].shape[0]\n",
    "#     idx = np.logical_and((v >= velmin).all(axis = 1), (v <= velmax).all(axis = 1))\n",
    "#     skeep = np.sum(idx)\n",
    "#     sdrop = np.sum(idx == False)\n",
    "#     return s, skeep, sdrop\n",
    "\n",
    "# for a, b in idx.loc[revise_idx, :].groupby(level = idx.index.names[2:-1]):\n",
    "#     pass#.describe()\n",
    "\n",
    "# x = revise_idx.droplevel(6).drop_duplicates()\n",
    "# idx.loc[slice(x, :), :]\n",
    "\n",
    "# idx.loc[revise_idx, :]\n",
    "# idx.groupby(level = idx.index.names[2:-1]).describe()\n",
    "# .loc[:, 'r'] = 'review'\n",
    "\n",
    "# mix = pd.IndexSlice\n",
    "# pd.concat((idx.loc[mix[[*t], :]] for t in x)) \n",
    "\n",
    "# mix = pd.IndexSlice\n",
    "# pd.concat((idx.loc[mix[[*t], :]] for t in x)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
