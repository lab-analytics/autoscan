{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, shutil, warnings, string\n",
    "import pandas as pd\n",
    "import deepdish as dp\n",
    "import numpy as np\n",
    "import re\n",
    "import fnmatch\n",
    "\n",
    "# libs for plotting\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models.tools import HoverTool\n",
    "from bokeh.models import LinearColorMapper, ColorBar, BasicTicker, Select, PrintfTickFormatter\n",
    "from bokeh.transform import jitter\n",
    "from bokeh.sampledata.commits import data\n",
    "from bokeh.palettes import brewer\n",
    "import holoviews\n",
    "\n",
    "# libs for stats\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# print in notebok\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "labutilspath = '/sandbox/dev/lab_utils/'\n",
    "sys.path.append(labutilspath)\n",
    "from _helpers.basics import basic_info\n",
    "info = basic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/media/sanromd/data/lab/data/characterization/autoscan/'\n",
    "datapath_processed = 'processed'\n",
    "datapath_raw       = 'raw'\n",
    "datapath_generic   = '_generic*'\n",
    "datapath_fluids    = '_fluids'\n",
    "datapath_analysis  = '_analysis'\n",
    "datapath_exclude   = ['_special-studies', 'special_studies', '_special_studies', '_unsorted',\n",
    "                      datapath_raw,datapath_generic,datapath_analysis,'*layout*']\n",
    "files_exclude      = ['.*','_*','*.asd','*.tcl']\n",
    "files_include      = ['*.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List and correct filenames (enforce patterns)\n",
    "\n",
    "1. Read the _csv_ files in the `datapath`\n",
    "1. Find whether each file has the word `before` or `after`\n",
    "1. If not, then add `before` as default\n",
    "1. Fix all file names such that they follow `probe-before|after-side.csv` \n",
    "1. Create a dataframe with a list of all measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rename_file(root,fname_old, fname_new):\n",
    "    oldname = os.path.join(root,fname_old)\n",
    "    newname = os.path.join(root,fname_new)\n",
    "    shutil.move(oldname,newname)\n",
    "    return\n",
    "\n",
    "def _get_sides(x):\n",
    "    s = re.findall(r'[a-z]+[.]',x)\n",
    "    side = None\n",
    "    if len(s)>0:\n",
    "        if not re.findall('before|after|map',s[-1]):\n",
    "            side = s[-1].split('.')[0]\n",
    "    return side\n",
    "\n",
    "def add_before_fname(fname,root,dryrun=False, debug=False):\n",
    "    name2 = re.sub(r'(perm|vel|impulse|ftir).*([a-z]+)[.]([a-z]+)',r'\\1_before_\\2.\\3',fname)\n",
    "    if debug: print(root.split('/')[len(datapath.split('/'))-1], fname, name2, sep='\\t')\n",
    "    if not dryrun:\n",
    "        _rename_file(root, fname, name2)\n",
    "    return name2\n",
    "\n",
    "def swap_instance_fname(fname,root,dryrun=False, debug=False):\n",
    "    name2 = re.sub(r'(perm|vel|impulse|ftir)(_|-)([a-z]+|[0-9]+)(_|-).*(before|after).*[.]([a-z]+)',\n",
    "                   r'\\1_\\5_\\3.\\6',\n",
    "                   fname)\n",
    "    if debug: print('swaping', root.split('/')[len(datapath.split('/'))-1], fname,name2,sep='\\t')\n",
    "    if not dryrun:\n",
    "        _rename_file(root, fname, name2)\n",
    "    return name2\n",
    "\n",
    "def check_autoscan_fname(fname, root, dryrun=False, debug=False):\n",
    "    instance = None\n",
    "    if (not 'before' in fname) and (not 'after' in fname):\n",
    "        warnings.warn('before or after not found in ' + \n",
    "                      root.split('/')[len(datapath.split('/'))-1] + fname)\n",
    "        fname = add_before_fname(fname, root, dryrun=dryrun, debug=debug)\n",
    "    instance = re.findall('before|after',fname)\n",
    "    if len(instance)==1:\n",
    "        instance = instance[0]\n",
    "        if debug: print(root.split('/')[len(datapath.split('/'))-1], instance, fname, sep='\\t')\n",
    "        if len(fname.split('_'))>2:\n",
    "            tst = re.match(r'(perm|vel|impulse|ftir)(-|_)(before|after).*([a-z]+)[.]([a-z]+)', fname)\n",
    "            if tst is None:\n",
    "                fname = swap_instance_fname(fname, root, dryrun=dryrun, debug=debug)\n",
    "    else:\n",
    "        if len(instance)>1:\n",
    "            warning = fname + ' has more than one instance: ' + ', '.join(instance)\n",
    "            warning = warning + ' and cannot choose! \\n check ' + os.path.join(root,fname) \n",
    "        if len(instance)==0:\n",
    "            warning = fname + ' not in either category (!) \\n please review!'\n",
    "        warnings.warn(warning)\n",
    "    return instance, fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file wrangling & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dryrun = True\n",
    "debug  = False\n",
    "\n",
    "excludes = r'|'.join([fnmatch.translate(x) for x in files_exclude]) or r'$.'\n",
    "includes = r'|'.join([fnmatch.translate(x) for x in files_include]) or r'$.'\n",
    "\n",
    "df = pd.DataFrame(columns=['probe','sample_tag','subsample_tag','side','instance','fname','relroot'])\n",
    "\n",
    "for root, dirs, files in os.walk(datapath):\n",
    "    [dirs.remove(d) for d in list(dirs) if d in datapath_exclude]\n",
    "    files = [f for f in files if not re.match(excludes, f)]\n",
    "    files = [f for f in files if re.match(includes, f)]\n",
    "    for fname in files:\n",
    "        #if debug: print(root.split('/')[len(datapath.split('/'))-1], fname)\n",
    "        instance,fname = check_autoscan_fname(fname, root, dryrun=dryrun, debug=debug)\n",
    "        # determine sample name\n",
    "        sample_tag = root.split('/')[len(datapath.split('/'))-1]\n",
    "        # determine if the file is subsample or sample\n",
    "        subornot = re.findall('subsample',root)\n",
    "        if len(subornot)==1:\n",
    "            subornot = subornot[0]\n",
    "            subsample_tag = root.split('/')[-2]\n",
    "        else:\n",
    "            subsample_tag = ''\n",
    "        side = ''\n",
    "        if len(fname.split('-'))>2:\n",
    "            side = fname.split('-')[-1].lower().replace('.csv','')\n",
    "\n",
    "        df = df.append({\n",
    "            'probe':re.findall(r'perm|vel|impulse|ftir',fname)[0],\n",
    "            'sample_tag':sample_tag,\n",
    "            'subsample_tag':subsample_tag,\n",
    "            'side':side,\n",
    "            'instance':instance,\n",
    "            'fname':fname,\n",
    "            'relroot':os.path.relpath(os.path.join(root,fname),start=datapath)\n",
    "        }, ignore_index=True)\n",
    "df.sort_values(by=['sample_tag','subsample_tag','probe','side'], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assign information\n",
    "get the information from basic info to know the `sample code` and `family`. This information will be used at a later stage for analysis (classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sample_code'] = df['sample_tag'].apply(lambda x: info.rock_dict[x.split('_')[0]]['code'])\n",
    "df['sample_family'] = df['sample_tag'].apply(lambda x: info.rock_dict[x.split('_')[0]]['family'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save\n",
    "1. add a `link` column to enable direct access\n",
    "1. save the csv file without the `link` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create link column and save\n",
    "df['link'] = df['relroot'].apply(lambda x: '<a href=\"./autoscan/{0}\">link</a>'.format(x))\n",
    "df.to_html(os.path.join('/media/sanromd/data/lab/data/characterization','autoscan.html'), \n",
    "           na_rep='-', escape=False)\n",
    "\n",
    "# save all columns except link\n",
    "df.loc[:, ['sample_tag', 'subsample_tag','sample_code', 'sample_family',\n",
    "                'probe', 'side', 'instance', 'fname', 'relroot']].to_csv(os.path.join(datapath,'summary.csv'),\n",
    "                                                                         index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine & recover information per probe\n",
    "1. Create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define probe of interest\n",
    "probe = 'perm'\n",
    "instance = 'before'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the summary dataframe with all measurements\n",
    "df_summary = pd.read_csv(os.path.join(datapath,'summary.csv'))\n",
    "df_summary['side'] = df_summary['fname'].apply(lambda x: _get_sides(x))\n",
    "df_probe = df_summary.loc[(df_summary['probe']==probe) & (df_summary['instance']==instance),:]\n",
    "df_probe = df_probe.drop(columns=['probe','instance']).copy()\n",
    "df_probe.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_settings = {\n",
    "    'perm':{\n",
    "        'usecols':[0,1,2,6,12],\n",
    "        'skiprows':7,\n",
    "        'names':['x','y','perm','meas_code','tile'],\n",
    "        'tip':['perm'],\n",
    "        'h':3\n",
    "    },\n",
    "    'impulse':{\n",
    "        'usecols':[0,1,2,3],\n",
    "        'skiprows':7,\n",
    "        'names':['x','y','e_star','tile'],\n",
    "        'tip':['e_start'],\n",
    "        'h':3\n",
    "    },\n",
    "    'vel':{\n",
    "        'usecols':[0,1,3,6,9,10],\n",
    "        'skiprows':7,\n",
    "        'names':['x','y','vp','vs','tile','direction'],\n",
    "        'tip':['vp','vs'],\n",
    "        'h':4\n",
    "    },\n",
    "    'fitr':{\n",
    "        'usecols':None,\n",
    "        'skiprows':2,\n",
    "        'names':None,\n",
    "        'tip':['ftir'],\n",
    "        'h':0\n",
    "    }\n",
    "    \n",
    "}\n",
    "def read_data(probe, fpath):\n",
    "    df = pd.read_csv(fpath, \n",
    "               usecols=probe_settings[probe]['usecols'],\n",
    "               skiprows=probe_settings[probe]['skiprows'],\n",
    "               names=probe_settings[probe]['names'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v</th>\n",
       "      <th>perm_c</th>\n",
       "      <th>meas_code</th>\n",
       "      <th>tile</th>\n",
       "      <th>side</th>\n",
       "      <th>code</th>\n",
       "      <th>family</th>\n",
       "      <th>tag</th>\n",
       "      <th>sub_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.02348</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>sg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>wsg_004</td>\n",
       "      <td>plugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>1.80530</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>sg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>wsg_004</td>\n",
       "      <td>plugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1.80362</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>sg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>wsg_004</td>\n",
       "      <td>plugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>1.07790</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>sg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>wsg_004</td>\n",
       "      <td>plugs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v   perm_c  meas_code  tile  side code     family      tag sub_tag\n",
       "0    0  1.02348          6     1  None   sg  sandstone  wsg_004   plugs\n",
       "1   50  1.80530          6     2  None   sg  sandstone  wsg_004   plugs\n",
       "2  100  1.80362          6     3  None   sg  sandstone  wsg_004   plugs\n",
       "3  150  1.07790          6     4  None   sg  sandstone  wsg_004   plugs"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v', 'perm_c', 'meas_code', 'tile']"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name_ordered + probe_settings[probe]['names'][h:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perm_c', 'v'}"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(col_names).difference(probe_settings[probe]['names'][h:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah_001/subsamples/s1/processed/perm_before.csv\n",
      "bg_003/processed/perm_before_a.csv\n",
      "bg_003/processed/perm_before_b.csv\n",
      "bg_003/processed/perm_before_c.csv\n",
      "bg_003/processed/perm_before_d.csv\n",
      "bg_003/subsamples/s1/processed/perm_before.csv\n",
      "bg_004/subsamples/line_01/processed/perm_before.csv\n",
      "bg_004/subsamples/s1/processed/perm_before.csv\n",
      "bg_004/subsamples/s2/processed/perm_before.csv\n",
      "bg_006/processed/perm_before_a.csv\n",
      "bg_006/processed/perm_before_b.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sandbox/opt/share/anaconda/lib/python3.7/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg_006/processed/perm_before_c.csv\n",
      "bg_006/processed/perm_before_d.csv\n",
      "bg_007/processed/perm_before_a.csv\n",
      "bg_007/processed/perm_before_b.csv\n",
      "bg_007/processed/perm_before_c.csv\n",
      "bg_007/processed/perm_before_d.csv\n",
      "bg_008/processed/perm_before_a.csv\n",
      "bg_008/processed/perm_before_b.csv\n",
      "bg_008/processed/perm_before_c.csv\n",
      "bg_008/processed/perm_before_d.csv\n",
      "bg_009/processed/perm_before_a.csv\n",
      "bg_009/processed/perm_before_b.csv\n",
      "bg_009/processed/perm_before_c.csv\n",
      "bg_009/processed/perm_before_d.csv\n",
      "bg_009/processed/perm_before_zbottom.csv\n",
      "bg_009/processed/perm_before_ztop.csv\n",
      "bg_011/processed/perm_before_a.csv\n",
      "bg_011/processed/perm_before_b.csv\n",
      "bg_011/processed/perm_before_c.csv\n",
      "bg_011/processed/perm_before_d.csv\n",
      "bg_012/processed/perm_before_a.csv\n",
      "bg_012/processed/perm_before_b.csv\n",
      "bg_012/processed/perm_before_c.csv\n",
      "bg_012/processed/perm_before_d.csv\n",
      "bg_019/subsamples/s10/processed/perm_before.csv\n",
      "bg_019/subsamples/s9/processed/perm_before.csv\n",
      "lssw_005/processed/perm_before_a.csv\n",
      "lssw_005/processed/perm_before_b.csv\n",
      "lssw_005/processed/perm_before_c.csv\n",
      "lssw_005/processed/perm_before_d.csv\n",
      "lssw_005/processed/perm_before_e.csv\n",
      "lssw_005/processed/perm_before_f.csv\n",
      "lssw_006/processed/perm_before_a.csv\n",
      "lssw_006/processed/perm_before_b.csv\n",
      "lssw_006/processed/perm_before_c.csv\n",
      "lssw_006/processed/perm_before_d.csv\n",
      "lssw_006/processed/perm_before_e.csv\n",
      "lssw_006/processed/perm_before_f.csv\n",
      "lssw_007/processed/perm_before_a.csv\n",
      "lssw_007/processed/perm_before_b.csv\n",
      "lssw_007/processed/perm_before_c.csv\n",
      "lssw_007/processed/perm_before_d.csv\n",
      "lssw_007/processed/perm_before_e.csv\n",
      "lssw_007/processed/perm_before_f.csv\n",
      "lssw_008/processed/perm_before_a.csv\n",
      "lssw_008/processed/perm_before_b.csv\n",
      "lssw_008/processed/perm_before_c.csv\n",
      "lssw_008/processed/perm_before_d.csv\n",
      "lssw_008/processed/perm_before_e.csv\n",
      "lssw_008/processed/perm_before_f.csv\n",
      "lssy_010/processed/perm_before_a.csv\n",
      "lssy_010/processed/perm_before_b.csv\n",
      "lssy_010/processed/perm_before_c.csv\n",
      "lssy_010/processed/perm_before_d.csv\n",
      "lssy_010/processed/perm_before_e.csv\n",
      "lssy_010/processed/perm_before_f.csv\n",
      "lssy_011/subsamples/s1/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s1/processed/perm_before_b.csv\n",
      "lssy_011/subsamples/s1/processed/perm_before_c.csv\n",
      "lssy_011/subsamples/s1/processed/perm_before_d.csv\n",
      "lssy_011/subsamples/s1_highres/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s2/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s2/processed/perm_before_b.csv\n",
      "lssy_011/subsamples/s2/processed/perm_before_c.csv\n",
      "lssy_011/subsamples/s2/processed/perm_before_d.csv\n",
      "lssy_011/subsamples/s3/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s3/processed/perm_before_b.csv\n",
      "lssy_011/subsamples/s3/processed/perm_before_c.csv\n",
      "lssy_011/subsamples/s3/processed/perm_before_d.csv\n",
      "lssy_011/subsamples/s4/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s4/processed/perm_before_b.csv\n",
      "lssy_011/subsamples/s4/processed/perm_before_c.csv\n",
      "lssy_011/subsamples/s4/processed/perm_before_d.csv\n",
      "lssy_014/processed/perm_before_a.csv\n",
      "lssy_014/processed/perm_before_b.csv\n",
      "lssy_014/processed/perm_before_c.csv\n",
      "lssy_014/processed/perm_before_d.csv\n",
      "lssy_014/processed/perm_before_e.csv\n",
      "lssy_014/processed/perm_before_f.csv\n",
      "lssy_015/processed/perm_before_a.csv\n",
      "lssy_015/processed/perm_before_b.csv\n",
      "lssy_015/processed/perm_before_c.csv\n",
      "lssy_015/processed/perm_before_d.csv\n",
      "lssy_015/processed/perm_before_e.csv\n",
      "lssy_015/processed/perm_before_f.csv\n",
      "lssy_016/processed/perm_before_a.csv\n",
      "lssy_016/processed/perm_before_b.csv\n",
      "lssy_016/processed/perm_before_c.csv\n",
      "lssy_016/processed/perm_before_d.csv\n",
      "lssy_016/processed/perm_before_e.csv\n",
      "lssy_016/processed/perm_before_f.csv\n",
      "lssy_026/processed/perm_before_a.csv\n",
      "lssy_026/processed/perm_before_b.csv\n",
      "lssy_026/processed/perm_before_c.csv\n",
      "lssy_026/processed/perm_before_d.csv\n",
      "lssy_026/processed/perm_before_e.csv\n",
      "lssy_026/processed/perm_before_f.csv\n",
      "lssy_027/processed/perm_before_a.csv\n",
      "lssy_027/processed/perm_before_b.csv\n",
      "lssy_027/processed/perm_before_c.csv\n",
      "lssy_027/processed/perm_before_d.csv\n",
      "lssy_027/processed/perm_before_e.csv\n",
      "lssy_027/processed/perm_before_f.csv\n",
      "lssy_028/subsamples/s1/processed/perm_before.csv\n",
      "lssy_028/subsamples/s2/processed/perm_before.csv\n",
      "mb_001/processed/perm_before_a.csv\n",
      "mb_001/processed/perm_before_b.csv\n",
      "mb_001/processed/perm_before_c.csv\n",
      "mb_001/processed/perm_before_d.csv\n",
      "mb_001/processed/perm_before_zbottom.csv\n",
      "mb_001/processed/perm_before_ztop.csv\n",
      "sh_001/processed/perm_before.csv\n",
      "sh_006/processed/perm_before_a.csv\n",
      "sh_006/processed/perm_before_b.csv\n",
      "sh_006/processed/perm_before_c.csv\n",
      "sh_006/processed/perm_before_d.csv\n",
      "sh_006/processed/perm_before_zbottom.csv\n",
      "sh_006/processed/perm_before_ztop.csv\n",
      "sh_007/processed/perm_before_a.csv\n",
      "sh_007/processed/perm_before_b.csv\n",
      "sh_007/processed/perm_before_c.csv\n",
      "sh_007/processed/perm_before_d.csv\n",
      "sh_007/processed/perm_before_zbottom.csv\n",
      "sh_007/processed/perm_before_ztop.csv\n",
      "sh_008/processed/perm_before_a.csv\n",
      "sh_008/processed/perm_before_b.csv\n",
      "sh_008/processed/perm_before_c.csv\n",
      "sh_008/processed/perm_before_d.csv\n",
      "sh_008/processed/perm_before_zbottom.csv\n",
      "sh_008/processed/perm_before_ztop.csv\n",
      "wsg_002/subsamples/plugs/processed/perm_before.csv\n",
      "wsg_003/subsamples/plugs/processed/perm_before.csv\n",
      "wsg_004/subsamples/plugs/processed/perm_before.csv\n"
     ]
    }
   ],
   "source": [
    "pool  = []\n",
    "lines = []\n",
    "\n",
    "for s in df_probe.iterrows():\n",
    "    root = s[-1]['relroot']\n",
    "    if not re.findall('analysis|map|heat|postprocess',root): \n",
    "        print(root)\n",
    "        # make the paths necessary to save the data\n",
    "        samplepath = os.path.sep.join(root.split('/')[:-2])\n",
    "        # create a _postprocessed directory in each sample to store the corresponding data\n",
    "        postprocesspath = os.path.join(datapath, s[-1]['sample_tag'],'_postprocessed')\n",
    "        if not os.path.exists(postprocesspath):\n",
    "            os.mkdir(postprocesspath)\n",
    "\n",
    "        # generate fname for files per sample\n",
    "        fname = '_'.join([probe, instance, str(s[-1]['subsample_tag']), str(s[-1]['side'])])\n",
    "        fname = fname.replace('_nan','')\n",
    "        fname = fname.replace('_None','')\n",
    "\n",
    "        # load the probe data\n",
    "        fpath = os.path.join(datapath,root)\n",
    "        dperm = read_data(probe, fpath)\n",
    "        # get rid of bad measurements (infs, nans, and text)\n",
    "        dperm = dperm.apply(pd.to_numeric,errors='coerce').dropna()\n",
    "        dperm.replace(np.inf, np.nan, inplace = True)\n",
    "        dperm.dropna(inplace = True)\n",
    "        # reset index \n",
    "        dperm.reset_index(inplace = True, drop = True)\n",
    "\n",
    "        # reset x,y offset to zero\n",
    "        dperm.iloc[:,:2] = dperm.iloc[:,:2] - dperm.iloc[:,:2].min() \n",
    "\n",
    "        # save probe data to postprocess\n",
    "        dperm.to_csv(os.path.join(postprocesspath,fname + '.csv'), index = False)\n",
    "\n",
    "        # pool data (we will assume that every point is idependent)\n",
    "        temp_dict_pool = {\n",
    "             'side':s[-1]['side'],\n",
    "             'code':s[-1]['sample_code'],\n",
    "             'family':s[-1]['sample_family'],\n",
    "             'tag':s[-1]['sample_tag'], \n",
    "             'sub_tag':s[-1]['subsample_tag']\n",
    "        }\n",
    "\n",
    "        for col in probe_settings[probe]['names'][2:]:\n",
    "            temp_dict_pool[col] = dperm[col].values\n",
    "        \n",
    "        df_temp_pool = pd.DataFrame(temp_dict_pool)\n",
    "        df_temp_pool = df_temp_pool.loc[:, probe_settings[probe]['names'][2:] + \n",
    "                                        ['side', 'code', 'family', 'tag', 'subtag']]\n",
    "        \n",
    "        pool.append(df_temp_pool)\n",
    "        \n",
    "        if not re.findall('ztop|zbottom',root):\n",
    "            h = probe_settings[probe]['h']\n",
    "            tip = probe_settings[probe]['tip']\n",
    "            \n",
    "            # get the slices and check which is the middle one\n",
    "            if (len(dperm.loc[:,'y'].unique())>1 and len(dperm.loc[:,'x'].unique())>1):\n",
    "                slice_along = np.int(dperm['x'].max()/dperm['y'].max() >= 1)\n",
    "                u = ['x','y'][slice_along]\n",
    "                v = ['x','y'][np.int(not np.bool(slice_along))]\n",
    "                slices_ini  = dperm.loc[:, u].unique()\n",
    "                slices_ini.sort()\n",
    "                if slices_ini.size > 1:\n",
    "                    median = slices_ini[slices_ini >= slices_ini.max()/2][0]\n",
    "                else:\n",
    "                    median = slices_ini[0]\n",
    "            else:\n",
    "                if len(dperm.loc[:,'y'].unique())==1: \n",
    "                    u = 'y'\n",
    "                    v = 'x'\n",
    "                else:\n",
    "                    u = 'x'\n",
    "                    v = 'y'\n",
    "                slices_ini = dperm.loc[:,u].unique()\n",
    "                median = dperm.loc[:,u].unique()[0]\n",
    "\n",
    "            # get center data\n",
    "            temp_list = []\n",
    "            col_names = []\n",
    "            \n",
    "            tip_names = [s+'_c' for s in tip]\n",
    "            col_names = ['v'] + tip_names + probe_settings[probe]['names'][h:]\n",
    "            col_name_ordered = ['v'] + tip_names\n",
    "            \n",
    "            for col in [v] + probe_settings[probe]['names'][2:]:\n",
    "                temp_list.append(dperm.loc[dperm.loc[:,u] == median, col].reset_index(drop = True))\n",
    "\n",
    "            if slices_ini.size>=3:\n",
    "                for p in tip:\n",
    "                    for k in slices_ini[[0,-1]]:    \n",
    "                        temp_list.append(dperm.loc[dperm.loc[:,u] == k,p].reset_index(drop = True))\n",
    "                col_names = col_names + [p + '_l', p + '_r']\n",
    "                col_name_ordered = col_name_ordered + [p + '_l', p + '_r']\n",
    "            \n",
    "            df_temp_lines = pd.concat(temp_list, axis = 1, ignore_index=True)\n",
    "            \n",
    "            df_temp_lines.columns =  col_names\n",
    "\n",
    "            # re order slices\n",
    "            col_name_ordered = col_name_ordered + probe_settings[probe]['names'][h:]\n",
    "            df_temp_lines = df_temp_lines.loc[:, col_name_ordered]\n",
    "\n",
    "            # add extra information (in case needed for statistical analysis)\n",
    "            df_temp_lines['side'] = s[-1]['side']\n",
    "            df_temp_lines['code'] = s[-1]['sample_code']\n",
    "            df_temp_lines['family'] = s[-1]['sample_family']\n",
    "            df_temp_lines['tag'] = s[-1]['sample_tag']\n",
    "            df_temp_lines['sub_tag'] = s[-1]['subsample_tag']\n",
    "\n",
    "            df_temp_lines.to_csv(os.path.join(postprocesspath, fname + '_lines.csv'), index = False)\n",
    "            lines.append(df_temp_lines)\n",
    "\n",
    "# concat and save\n",
    "df_pool = pd.concat(pool, ignore_index=True)\n",
    "df_lines = pd.concat(lines, ignore_index=True, sort=False)\n",
    "\n",
    "df_pool.to_csv(os.path.join(datapath, '_postprocessed', probe + '_pool.csv'), index = False)\n",
    "df_lines.to_csv(os.path.join(datapath, '_postprocessed', probe + '_lines.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah_001/subsamples/s1/processed/perm_before.csv\n",
      "bg_003/processed/perm_before_a.csv\n",
      "bg_003/processed/perm_before_b.csv\n",
      "bg_003/processed/perm_before_c.csv\n",
      "bg_003/processed/perm_before_d.csv\n",
      "bg_003/subsamples/s1/processed/perm_before.csv\n",
      "bg_004/subsamples/line_01/processed/perm_before.csv\n",
      "bg_004/subsamples/s1/processed/perm_before.csv\n",
      "bg_004/subsamples/s2/processed/perm_before.csv\n",
      "bg_006/processed/perm_before_a.csv\n",
      "bg_006/processed/perm_before_b.csv\n",
      "bg_006/processed/perm_before_c.csv\n",
      "bg_006/processed/perm_before_d.csv\n",
      "bg_007/processed/perm_before_a.csv\n",
      "bg_007/processed/perm_before_b.csv\n",
      "bg_007/processed/perm_before_c.csv\n",
      "bg_007/processed/perm_before_d.csv\n",
      "bg_008/processed/perm_before_a.csv\n",
      "bg_008/processed/perm_before_b.csv\n",
      "bg_008/processed/perm_before_c.csv\n",
      "bg_008/processed/perm_before_d.csv\n",
      "bg_009/processed/perm_before_a.csv\n",
      "bg_009/processed/perm_before_b.csv\n",
      "bg_009/processed/perm_before_c.csv\n",
      "bg_009/processed/perm_before_d.csv\n",
      "bg_009/processed/perm_before_zbottom.csv\n",
      "bg_009/processed/perm_before_ztop.csv\n",
      "bg_011/processed/perm_before_a.csv\n",
      "bg_011/processed/perm_before_b.csv\n",
      "bg_011/processed/perm_before_c.csv\n",
      "bg_011/processed/perm_before_d.csv\n",
      "bg_012/processed/perm_before_a.csv\n",
      "bg_012/processed/perm_before_b.csv\n",
      "bg_012/processed/perm_before_c.csv\n",
      "bg_012/processed/perm_before_d.csv\n",
      "bg_019/subsamples/s10/processed/perm_before.csv\n",
      "bg_019/subsamples/s9/processed/perm_before.csv\n",
      "lssw_005/processed/perm_before_a.csv\n",
      "lssw_005/processed/perm_before_b.csv\n",
      "lssw_005/processed/perm_before_c.csv\n",
      "lssw_005/processed/perm_before_d.csv\n",
      "lssw_005/processed/perm_before_e.csv\n",
      "lssw_005/processed/perm_before_f.csv\n",
      "lssw_006/processed/perm_before_a.csv\n",
      "lssw_006/processed/perm_before_b.csv\n",
      "lssw_006/processed/perm_before_c.csv\n",
      "lssw_006/processed/perm_before_d.csv\n",
      "lssw_006/processed/perm_before_e.csv\n",
      "lssw_006/processed/perm_before_f.csv\n",
      "lssw_007/processed/perm_before_a.csv\n",
      "lssw_007/processed/perm_before_b.csv\n",
      "lssw_007/processed/perm_before_c.csv\n",
      "lssw_007/processed/perm_before_d.csv\n",
      "lssw_007/processed/perm_before_e.csv\n",
      "lssw_007/processed/perm_before_f.csv\n",
      "lssw_008/processed/perm_before_a.csv\n",
      "lssw_008/processed/perm_before_b.csv\n",
      "lssw_008/processed/perm_before_c.csv\n",
      "lssw_008/processed/perm_before_d.csv\n",
      "lssw_008/processed/perm_before_e.csv\n",
      "lssw_008/processed/perm_before_f.csv\n",
      "lssy_010/processed/perm_before_a.csv\n",
      "lssy_010/processed/perm_before_b.csv\n",
      "lssy_010/processed/perm_before_c.csv\n",
      "lssy_010/processed/perm_before_d.csv\n",
      "lssy_010/processed/perm_before_e.csv\n",
      "lssy_010/processed/perm_before_f.csv\n",
      "lssy_011/subsamples/s1/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s1/processed/perm_before_b.csv\n",
      "lssy_011/subsamples/s1/processed/perm_before_c.csv\n",
      "lssy_011/subsamples/s1/processed/perm_before_d.csv\n",
      "lssy_011/subsamples/s1_highres/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s2/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s2/processed/perm_before_b.csv\n",
      "lssy_011/subsamples/s2/processed/perm_before_c.csv\n",
      "lssy_011/subsamples/s2/processed/perm_before_d.csv\n",
      "lssy_011/subsamples/s3/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s3/processed/perm_before_b.csv\n",
      "lssy_011/subsamples/s3/processed/perm_before_c.csv\n",
      "lssy_011/subsamples/s3/processed/perm_before_d.csv\n",
      "lssy_011/subsamples/s4/processed/perm_before_a.csv\n",
      "lssy_011/subsamples/s4/processed/perm_before_b.csv\n",
      "lssy_011/subsamples/s4/processed/perm_before_c.csv\n",
      "lssy_011/subsamples/s4/processed/perm_before_d.csv\n",
      "lssy_014/processed/perm_before_a.csv\n",
      "lssy_014/processed/perm_before_b.csv\n",
      "lssy_014/processed/perm_before_c.csv\n",
      "lssy_014/processed/perm_before_d.csv\n",
      "lssy_014/processed/perm_before_e.csv\n",
      "lssy_014/processed/perm_before_f.csv\n",
      "lssy_015/processed/perm_before_a.csv\n",
      "lssy_015/processed/perm_before_b.csv\n",
      "lssy_015/processed/perm_before_c.csv\n",
      "lssy_015/processed/perm_before_d.csv\n",
      "lssy_015/processed/perm_before_e.csv\n",
      "lssy_015/processed/perm_before_f.csv\n",
      "lssy_016/processed/perm_before_a.csv\n",
      "lssy_016/processed/perm_before_b.csv\n",
      "lssy_016/processed/perm_before_c.csv\n",
      "lssy_016/processed/perm_before_d.csv\n",
      "lssy_016/processed/perm_before_e.csv\n",
      "lssy_016/processed/perm_before_f.csv\n",
      "lssy_026/processed/perm_before_a.csv\n",
      "lssy_026/processed/perm_before_b.csv\n",
      "lssy_026/processed/perm_before_c.csv\n",
      "lssy_026/processed/perm_before_d.csv\n",
      "lssy_026/processed/perm_before_e.csv\n",
      "lssy_026/processed/perm_before_f.csv\n",
      "lssy_027/processed/perm_before_a.csv\n",
      "lssy_027/processed/perm_before_b.csv\n",
      "lssy_027/processed/perm_before_c.csv\n",
      "lssy_027/processed/perm_before_d.csv\n",
      "lssy_027/processed/perm_before_e.csv\n",
      "lssy_027/processed/perm_before_f.csv\n",
      "lssy_028/subsamples/s1/processed/perm_before.csv\n",
      "lssy_028/subsamples/s2/processed/perm_before.csv\n",
      "mb_001/processed/perm_before_a.csv\n",
      "mb_001/processed/perm_before_b.csv\n",
      "mb_001/processed/perm_before_c.csv\n",
      "mb_001/processed/perm_before_d.csv\n",
      "mb_001/processed/perm_before_zbottom.csv\n",
      "mb_001/processed/perm_before_ztop.csv\n",
      "sh_001/processed/perm_before.csv\n",
      "sh_006/processed/perm_before_a.csv\n",
      "sh_006/processed/perm_before_b.csv\n",
      "sh_006/processed/perm_before_c.csv\n",
      "sh_006/processed/perm_before_d.csv\n",
      "sh_006/processed/perm_before_zbottom.csv\n",
      "sh_006/processed/perm_before_ztop.csv\n",
      "sh_007/processed/perm_before_a.csv\n",
      "sh_007/processed/perm_before_b.csv\n",
      "sh_007/processed/perm_before_c.csv\n",
      "sh_007/processed/perm_before_d.csv\n",
      "sh_007/processed/perm_before_zbottom.csv\n",
      "sh_007/processed/perm_before_ztop.csv\n",
      "sh_008/processed/perm_before_a.csv\n",
      "sh_008/processed/perm_before_b.csv\n",
      "sh_008/processed/perm_before_c.csv\n",
      "sh_008/processed/perm_before_d.csv\n",
      "sh_008/processed/perm_before_zbottom.csv\n",
      "sh_008/processed/perm_before_ztop.csv\n",
      "wsg_002/subsamples/plugs/processed/perm_before.csv\n",
      "wsg_003/subsamples/plugs/processed/perm_before.csv\n",
      "wsg_004/subsamples/plugs/processed/perm_before.csv\n"
     ]
    }
   ],
   "source": [
    "# pool  = []\n",
    "# lines = []\n",
    "\n",
    "# for s in df_probe.iterrows():\n",
    "#     root = s[-1]['relroot']\n",
    "#     if not re.findall('analysis|map|heat|postprocess',root): \n",
    "#         print(root)\n",
    "#         # make the paths necessary to save the data\n",
    "#         samplepath = os.path.sep.join(root.split('/')[:-2])\n",
    "#         # create a _postprocessed directory in each sample to store the corresponding data\n",
    "#         postprocesspath = os.path.join(datapath, s[-1]['sample_tag'],'_postprocessed')\n",
    "#         if not os.path.exists(postprocesspath):\n",
    "#             os.mkdir(postprocesspath)\n",
    "\n",
    "#         # generate fname for files per sample\n",
    "#         fname = '_'.join([probe, instance, str(s[-1]['subsample_tag']), str(s[-1]['side'])])\n",
    "#         fname = fname.replace('_nan','')\n",
    "#         fname = fname.replace('_None','')\n",
    "\n",
    "#         # load the probe data\n",
    "#         fpath = os.path.join(datapath,root)\n",
    "#         dperm = pd.read_csv(fpath, skiprows=7, usecols=[0,1,2,12], names = ['x','y',probe,'tile'])\n",
    "#         # get rid of bad measurements (infs, nans, and text)\n",
    "#         dperm = dperm.apply(pd.to_numeric,errors='coerce').dropna()\n",
    "#         dperm.replace(np.inf, np.nan, inplace = True)\n",
    "#         dperm.dropna(inplace = True)\n",
    "#         # reset index \n",
    "#         dperm.reset_index(inplace = True, drop = True)\n",
    "\n",
    "#         # reset x,y offset to zero\n",
    "#         dperm.iloc[:,:2] = dperm.iloc[:,:2] - dperm.iloc[:,:2].min() \n",
    "\n",
    "#         # save probe data to postprocess\n",
    "#         dperm.to_csv(os.path.join(postprocesspath,fname + '.csv'), index = False)\n",
    "\n",
    "#         # pool data (we will assume that every point is idependent)\n",
    "#         df_temp_pool = pd.DataFrame({probe:dperm[probe].values,\n",
    "#                                      'meas_code':dperm['meas_code']\n",
    "#                                      'tile':dperm['tile'].values,\n",
    "#                                      'side':s[-1]['side'],\n",
    "#                                      'code':s[-1]['sample_code'],\n",
    "#                                      'family':s[-1]['sample_family'],\n",
    "#                                      'tag':s[-1]['sample_tag'], \n",
    "#                                      'sub_tag':s[-1]['subsample_tag']})\n",
    "#         if probe == 'perm':\n",
    "        \n",
    "#         temp_dict_pool = {}\n",
    "#         for col in probe_settings[probe]['names'][2:]:\n",
    "#             temp_dict_pool[col] = dperm[col].values\n",
    "#         pool.append(df_temp_pool)\n",
    "        \n",
    "#         if not re.findall('ztop|zbottom',root):\n",
    "#             # get the slices and check which is the middle one\n",
    "#             if (len(dperm.loc[:,'y'].unique())>1 and len(dperm.loc[:,'x'].unique())>1):\n",
    "#                 slice_along = np.int(dperm['x'].max()/dperm['y'].max() >= 1)\n",
    "#                 u = ['x','y'][slice_along]\n",
    "#                 v = ['x','y'][np.int(not np.bool(slice_along))]\n",
    "#                 slices_ini  = dperm.loc[:, u].unique()\n",
    "#                 slices_ini.sort()\n",
    "#                 if slices_ini.size > 1:\n",
    "#                     median = slices_ini[slices_ini >= slices_ini.max()/2][0]\n",
    "#                 else:\n",
    "#                     median = slices_ini[0]\n",
    "#             else:\n",
    "#                 if len(dperm.loc[:,'y'].unique())==1: \n",
    "#                     u = 'y'\n",
    "#                     v = 'x'\n",
    "#                 else:\n",
    "#                     u = 'x'\n",
    "#                     v = 'y'\n",
    "#                 slices_ini = dperm.loc[:,u].unique()\n",
    "#                 median = dperm.loc[:,u].unique()[0]\n",
    "\n",
    "#             # get center data\n",
    "#             df_temp_lines = pd.concat([\n",
    "#                 dperm.loc[dperm.loc[:,u] == median, v].reset_index(drop = True),\n",
    "#                 dperm.loc[dperm.loc[:,u] == median,probe].reset_index(drop = True),\n",
    "#                 dperm.loc[dperm.loc[:,u] == median,'tile'].reset_index(drop = True)],\n",
    "#                 axis = 1, ignore_index=True)\n",
    "#             df_temp_lines.columns = ['v',probe + '_c', 'tile']\n",
    "            \n",
    "#             # add edges\n",
    "#             if slices_ini.size>=3:\n",
    "#                 df_temp_lines = pd.concat([df_temp_lines,\n",
    "#                         dperm.loc[dperm.loc[:,u] == slices_ini[0],probe].reset_index(drop = True),\n",
    "#                         dperm.loc[dperm.loc[:,u] == slices_ini[-1],probe].reset_index(drop = True)],\n",
    "#                         axis = 1, ignore_index = True)\n",
    "\n",
    "#                 df_temp_lines.columns = ['v',probe + '_c', 'tile', probe+ '_l', probe + '_r']\n",
    "\n",
    "#             # re order slices\n",
    "#             df_temp_lines = df_temp_lines.loc[:,['v', probe+'_c', probe+'_l',probe+'_r','tile']]\n",
    "\n",
    "#             # add extra information (in case needed for statistical analysis)\n",
    "#             df_temp_lines['side'] = s[-1]['side']\n",
    "#             df_temp_lines['code'] = s[-1]['sample_code']\n",
    "#             df_temp_lines['family'] = s[-1]['sample_family']\n",
    "#             df_temp_lines['tag'] = s[-1]['sample_tag']\n",
    "#             df_temp_lines['sub_tag'] = s[-1]['subsample_tag']\n",
    "\n",
    "#             df_temp_lines.to_csv(os.path.join(postprocesspath, fname + '_lines.csv'), index = False)\n",
    "#             lines.append(df_temp_lines)\n",
    "\n",
    "# # concat and save\n",
    "# df_pool = pd.concat(pool, ignore_index=True)\n",
    "# df_lines = pd.concat(lines, ignore_index=True)\n",
    "\n",
    "# df_pool.to_csv(os.path.join(datapath, '_postprocessed', probe + '_pool.csv'), index = False)\n",
    "# df_lines.to_csv(os.path.join(datapath, '_postprocessed', probe + '_lines.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_tag</th>\n",
       "      <th>subsample_tag</th>\n",
       "      <th>sample_code</th>\n",
       "      <th>sample_family</th>\n",
       "      <th>side</th>\n",
       "      <th>fname</th>\n",
       "      <th>relroot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg_003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>a</td>\n",
       "      <td>perm_before_a.csv</td>\n",
       "      <td>bg_003/processed/perm_before_a.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg_003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>b</td>\n",
       "      <td>perm_before_b.csv</td>\n",
       "      <td>bg_003/processed/perm_before_b.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg_003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>c</td>\n",
       "      <td>perm_before_c.csv</td>\n",
       "      <td>bg_003/processed/perm_before_c.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg_003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>d</td>\n",
       "      <td>perm_before_d.csv</td>\n",
       "      <td>bg_003/processed/perm_before_d.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bg_003</td>\n",
       "      <td>s1</td>\n",
       "      <td>bg</td>\n",
       "      <td>sandstone</td>\n",
       "      <td>None</td>\n",
       "      <td>perm_before.csv</td>\n",
       "      <td>bg_003/subsamples/s1/processed/perm_before.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_tag subsample_tag sample_code sample_family  side              fname  \\\n",
       "1     bg_003           NaN          bg     sandstone     a  perm_before_a.csv   \n",
       "2     bg_003           NaN          bg     sandstone     b  perm_before_b.csv   \n",
       "3     bg_003           NaN          bg     sandstone     c  perm_before_c.csv   \n",
       "4     bg_003           NaN          bg     sandstone     d  perm_before_d.csv   \n",
       "5     bg_003            s1          bg     sandstone  None    perm_before.csv   \n",
       "\n",
       "                                          relroot  \n",
       "1              bg_003/processed/perm_before_a.csv  \n",
       "2              bg_003/processed/perm_before_b.csv  \n",
       "3              bg_003/processed/perm_before_c.csv  \n",
       "4              bg_003/processed/perm_before_d.csv  \n",
       "5  bg_003/subsamples/s1/processed/perm_before.csv  "
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probe.loc[df_probe['sample_tag']=='bg_003',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp_lines = dperm.loc[\n",
    "#     dperm.iloc[:,slice_along] == median,:].iloc[:,[np.int(not np.bool(slice_along)),2,3]].copy()\n",
    "# df_temp_lines.reset_index(inplace = True, drop=True)\n",
    "# # rename slice column to avoid conflicts in the future\n",
    "# df_temp_lines.columns = ['u', probe, 'tile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-374-ab35ff04ca3f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-374-ab35ff04ca3f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dperm['tile'].unique(\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "dperm['tile'].unique("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check edges\n",
    "vals_index = dperm.iloc[:,slice_along] == median\n",
    "df_temp_lines = pd.DataFrame({\n",
    "    probe+'_center':dperm.loc[vals_index,probe].values,\n",
    "    'u':dperm.loc[vals_index,dperm.columns[np.int(not np.bool(slice_along))]].values,\n",
    "    'tile':dperm.loc[vals_index,'tile'].values})\n",
    "\n",
    "# add edges\n",
    "if slices_ini.size>=3:\n",
    "    df_temp_lines.loc[:,probe+'_left'] = dperm.loc[dperm.iloc[:,slice_along] == slices_ini[0],probe].values\n",
    "    df_temp_lines.loc[:,probe+'_right'] = dperm.loc[dperm.iloc[:,slice_along] == slices_ini[-1],probe].values\n",
    "\n",
    "# re order slices\n",
    "df_temp_lines = df_temp_lines.loc[:,['u', probe+'_center', probe+'_left',probe+'_right','tile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
