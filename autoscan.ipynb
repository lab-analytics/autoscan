{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, shutil, warnings, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "labutilspath = '/sandbox/dev/lab_utils/'\n",
    "sys.path.append(labutilspath)\n",
    "from _helpers.basics import basic_info\n",
    "info = basic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/media/sanromd/data/lab/data/characterization/autoscan/'\n",
    "datapath_processed = 'processed'\n",
    "datapath_raw       = 'raw'\n",
    "datapath_generic   = '_generic*'\n",
    "datapath_fluids    = '_fluids'\n",
    "datapath_analysis  = '_analysis'\n",
    "datapath_exclude   = ['_special-studies', 'special_studies', '_special_studies', '_unsorted',\n",
    "                      datapath_raw, datapath_generic, datapath_analysis, '*layout*', '_postprocessed']\n",
    "files_exclude      = ['.*','_*','*.asd','*.tcl', 'summary*','*map*']\n",
    "files_include      = ['*.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List and correct filenames (enforce patterns)\n",
    "\n",
    "1. Read the _csv_ files in the `datapath`\n",
    "1. Find whether each file has the word `before` or `after`\n",
    "1. If not, then add `before` as default\n",
    "1. Fix all file names such that they follow `probe-before|after-side.csv` \n",
    "1. Create a dataframe with a list of all measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_settings = {\n",
    "    'perm':{\n",
    "        'usecols':[0,1,2,6,12],\n",
    "        'skiprows':7,\n",
    "        'names':['x','y','perm','meas_code','tile'],\n",
    "        'tip':['perm'],\n",
    "        'h':3\n",
    "    },\n",
    "    'impulse':{\n",
    "        'usecols':[0,1,2,3],\n",
    "        'skiprows':7,\n",
    "        'names':['x','y','e_star','tile'],\n",
    "        'tip':['e_star'],\n",
    "        'h':3\n",
    "    },\n",
    "    'vel':{\n",
    "        'usecols':[0,1,3,6,9,10],\n",
    "        'skiprows':7,\n",
    "        'names':['x','y','vp','vs','tile','direction'],\n",
    "        'tip':['vp','vs'],\n",
    "        'h':4\n",
    "    },\n",
    "    'ftir':{\n",
    "        'usecols':None,\n",
    "        'skiprows':0,\n",
    "        'names':['x','y']+['l_'+str(int(x)) for x in np.linspace(1,1752,1752)],\n",
    "        'tip':['l_'+str(int(x)) for x in np.linspace(1,1752,1752)],\n",
    "        'h':0\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "def _rename_file(root,fname_old, fname_new):\n",
    "    oldname = os.path.join(root,fname_old)\n",
    "    newname = os.path.join(root,fname_new)\n",
    "    shutil.move(oldname,newname)\n",
    "    return\n",
    "\n",
    "def _get_refindall(pattern, string):\n",
    "    v = None\n",
    "    s = re.findall(pattern, string)\n",
    "    if len(s)>0:\n",
    "        v = s[0]\n",
    "    return v\n",
    "\n",
    "def _get_sides(x):\n",
    "    side = _get_refindall(r'(before|after)_([a-z]+|[0-9]+)[.]', x)\n",
    "    if side is not None:\n",
    "        side = side[-1]\n",
    "    return side\n",
    "\n",
    "def _get_subsample(x):\n",
    "    sub = _get_refindall(r'.*sub[a-z]+[/]([a-z]+[0-9]+)[/]',x)\n",
    "    return sub\n",
    "\n",
    "def _get_probename(x):\n",
    "    probe = _get_refindall(r'(perm|vel|impulse|ftir)',x)\n",
    "    return probe\n",
    "\n",
    "def _get_instance(x):\n",
    "    instance = _get_refindall(r'(before|after)', x)\n",
    "    return instance\n",
    "\n",
    "def _get_rockinfo(x, key=None):\n",
    "    s = info.rock_dict[x.split('_')[0]][key]\n",
    "    return s\n",
    "\n",
    "def add_before_fname(fname,root,dryrun=False, debug=False):\n",
    "    name2 = re.sub(r'(perm|vel|impulse|ftir).*([a-z]+)[.]([a-z]+)',r'\\1_before_\\2.\\3',fname)\n",
    "    if debug: print(root.split('/')[len(datapath.split('/'))-1], fname, name2, sep='\\t')\n",
    "    if not dryrun:\n",
    "        _rename_file(root, fname, name2)\n",
    "    return name2\n",
    "\n",
    "def swap_instance_fname(fname,root,dryrun=False, debug=False):\n",
    "    name2 = re.sub(r'(perm|vel|impulse|ftir)(_|-)([a-z]+|[0-9]+)(_|-).*(before|after).*[.]([a-z]+)',\n",
    "                   r'\\1_\\5_\\3.\\6',\n",
    "                   fname)\n",
    "    if debug: print('swaping', root.split('/')[len(datapath.split('/'))-1], fname,name2,sep='\\t')\n",
    "    if not dryrun:\n",
    "        _rename_file(root, fname, name2)\n",
    "    return name2\n",
    "\n",
    "def check_autoscan_fname(fname, root, dryrun=False, debug=False):\n",
    "    instance = None\n",
    "    if (not 'before' in fname) and (not 'after' in fname):\n",
    "        warnings.warn('before or after not found in ' + \n",
    "                      root.split('/')[len(datapath.split('/'))-1] + fname)\n",
    "        fname = add_before_fname(fname, root, dryrun=dryrun, debug=debug)\n",
    "    instance = re.findall('before|after',fname)\n",
    "    if len(instance)==1:\n",
    "        instance = instance[0]\n",
    "        if debug: print(root.split('/')[len(datapath.split('/'))-1], instance, fname, sep='\\t')\n",
    "        if len(fname.split('_'))>2:\n",
    "            tst = re.match(r'(perm|vel|impulse|ftir)(-|_)(before|after).*([a-z]+)[.]([a-z]+)', fname)\n",
    "            if tst is None:\n",
    "                fname = swap_instance_fname(fname, root, dryrun=dryrun, debug=debug)\n",
    "    else:\n",
    "        if len(instance)>1:\n",
    "            warning = fname + ' has more than one instance: ' + ', '.join(instance)\n",
    "            warning = warning + ' and cannot choose! \\n check ' + os.path.join(root,fname) \n",
    "        if len(instance)==0:\n",
    "            warning = fname + ' not in either category (!) \\n please review!'\n",
    "        warnings.warn(warning)\n",
    "    return fname\n",
    "\n",
    "def _vel_direction(x):\n",
    "    d = -1\n",
    "    if x == 'velax':\n",
    "        d = 1\n",
    "    return d\n",
    "\n",
    "def read_data(probe, fpath, reset_offset = True, save = False, savepath = './', savename = None):\n",
    "    df = pd.read_csv(fpath, \n",
    "               usecols=probe_settings[probe]['usecols'],\n",
    "               skiprows=probe_settings[probe]['skiprows'],\n",
    "               names=probe_settings[probe]['names'])\n",
    "    \n",
    "    if probe == 'vel':\n",
    "        df['direction'] = df['direction'].apply(lambda x: _vel_direction(x))\n",
    "    # get rid of bad measurements (infs, nans, and text)\n",
    "    df = df.apply(pd.to_numeric,errors='coerce').dropna()\n",
    "    df.replace(np.inf, np.nan, inplace = True)\n",
    "    df.dropna(inplace = True)\n",
    "    # reset index \n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    # reset x,y offset to zero\n",
    "    if reset_offset:\n",
    "        df.iloc[:,:2] = df.iloc[:,:2] - df.iloc[:,:2].min()\n",
    "    \n",
    "    # save data if needed\n",
    "    if save:\n",
    "        if savename is None: savename = probe + '.csv'\n",
    "        df.to_csv(os.path.join(savepath,savename), index = False)   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file wrangling & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dryrun = True\n",
    "debug  = False\n",
    "\n",
    "excludes = r'|'.join([fnmatch.translate(x) for x in files_exclude]) or r'$.'\n",
    "includes = r'|'.join([fnmatch.translate(x) for x in files_include]) or r'$.'\n",
    "\n",
    "fs = []\n",
    "rs = []\n",
    "for root, dirs, files in os.walk(datapath):\n",
    "    [dirs.remove(d) for d in list(dirs) if d in datapath_exclude]\n",
    "    files = [f for f in files if not re.match(excludes, f)]\n",
    "    files = [f for f in files if re.match(includes, f)]\n",
    "    for fname in files:\n",
    "        fname = check_autoscan_fname(fname, root, dryrun=dryrun, debug=debug)\n",
    "        fs.append(fname)\n",
    "        rs.append(os.path.relpath(os.path.join(root,fname),start=datapath))\n",
    "\n",
    "df = pd.DataFrame({'fname':fs, 'relroot':rs})\n",
    "df['sample_tag']    = df['relroot'].apply(lambda x: x.split('/')[0])\n",
    "df['subsample_tag'] = df['relroot'].apply(_get_subsample)\n",
    "\n",
    "df = pd.concat([df, \n",
    "                df.fname.apply(lambda s: pd.Series({'probe':_get_probename(s), \n",
    "                                                    'side':_get_sides(s), \n",
    "                                                    'instance':_get_instance(s)})),\n",
    "          ],\n",
    "          axis = 1, sort = False)\n",
    "\n",
    "for s in ['code', 'family']:\n",
    "    df['sample_' + s] = df['sample_tag'].apply(_get_rockinfo, key=s)\n",
    "\n",
    "df = df.loc[:, ['sample_tag', 'subsample_tag', 'side', 'sample_code', 'sample_family',\n",
    "                'probe', 'instance', 'fname', 'relroot']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save\n",
    "1. add a `link` column to enable direct access\n",
    "1. save the csv file without the `link` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create link column and save\n",
    "df['link'] = df['relroot'].apply(lambda x: '<a href=\"./autoscan/{0}\">link</a>'.format(x))\n",
    "df.to_html(os.path.join('/media/sanromd/data/lab/data/characterization','autoscan.html'), \n",
    "           na_rep='-', escape=False)\n",
    "\n",
    "# save all columns except link\n",
    "df.loc[:, ['sample_tag', 'subsample_tag','sample_code', 'sample_family',\n",
    "                'probe', 'side', 'instance', 'fname', 'relroot']].to_csv(os.path.join(datapath,'summary.csv'),\n",
    "                                                                         index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine & recover information per probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the summary dataframe with all measurements\n",
    "df_summary = pd.read_csv(os.path.join(datapath,'summary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ztop and zbottom\n",
    "t = df_summary['relroot'].apply(lambda x: len(re.findall(r'ztop|zbottom', x))==0)\n",
    "df_summary = df_summary.loc[t,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define probe of interest\n",
    "probe = 'ftir'\n",
    "instance = 'before'\n",
    "\n",
    "df_probe = df_summary.loc[(df_summary['probe']==probe) & (df_summary['instance']==instance),:].copy()\n",
    "df_probe = df_probe.drop(columns=['probe','instance']).copy()\n",
    "df_probe.reset_index(inplace=True, drop=True)\n",
    "\n",
    "pool  = []\n",
    "lines = []\n",
    "\n",
    "for s in df_probe.iterrows():\n",
    "    root = s[-1]['relroot']\n",
    "    print(root)\n",
    "    # make the paths necessary to save the data\n",
    "    samplepath = os.path.sep.join(root.split('/')[:-2])\n",
    "    # create a _postprocessed directory in each sample to store the corresponding data\n",
    "    postprocesspath = os.path.join(datapath, s[-1]['sample_tag'],'_postprocessed')\n",
    "    if not os.path.exists(postprocesspath):\n",
    "        os.mkdir(postprocesspath)\n",
    "\n",
    "    # generate fname for files per sample\n",
    "    fname = '_'.join([probe, instance, str(s[-1]['subsample_tag']), str(s[-1]['side'])])\n",
    "    fname = fname.replace('_nan','')\n",
    "    fname = fname.replace('_None','')\n",
    "\n",
    "    # load the probe data\n",
    "    fpath = os.path.join(datapath,root)\n",
    "    dperm = read_data(probe, fpath, reset_offset=True, \n",
    "                      save=True, savepath=postprocesspath, savename=fname+'.csv')\n",
    "    \n",
    "    if len(dperm)>0:\n",
    "        # pool data (we will assume that every point is idependent)\n",
    "        temp_dict_pool = {\n",
    "             'side':s[-1]['side'],\n",
    "             'code':s[-1]['sample_code'],\n",
    "             'family':s[-1]['sample_family'],\n",
    "             'tag':s[-1]['sample_tag'], \n",
    "             'sub_tag':s[-1]['subsample_tag']\n",
    "        }\n",
    "\n",
    "        for col in probe_settings[probe]['names'][2:]:\n",
    "            temp_dict_pool[col] = dperm[col].values\n",
    "\n",
    "        df_temp_pool = pd.DataFrame(temp_dict_pool)\n",
    "        df_temp_pool = df_temp_pool.loc[:, probe_settings[probe]['names'][2:] + \n",
    "                                        ['side', 'code', 'family', 'tag', 'subtag']]\n",
    "\n",
    "        pool.append(df_temp_pool)\n",
    "\n",
    "        h = probe_settings[probe]['h']\n",
    "        tip = probe_settings[probe]['tip']\n",
    "\n",
    "        # get the slices and check which is the middle one\n",
    "        if (len(dperm.loc[:,'y'].unique())>1 and len(dperm.loc[:,'x'].unique())>1):\n",
    "            slice_along = np.int(dperm['x'].max()/dperm['y'].max() >= 1)\n",
    "            u = ['x','y'][slice_along]\n",
    "            v = ['x','y'][np.int(not np.bool(slice_along))]\n",
    "            slices_ini  = dperm.loc[:, u].unique()\n",
    "            slices_ini.sort()\n",
    "            if slices_ini.size > 1:\n",
    "                median = slices_ini[slices_ini >= slices_ini.max()/2][0]\n",
    "            else:\n",
    "                median = slices_ini[0]\n",
    "        else:\n",
    "            if len(dperm.loc[:,'y'].unique())==1: \n",
    "                u = 'y'\n",
    "                v = 'x'\n",
    "            else:\n",
    "                u = 'x'\n",
    "                v = 'y'\n",
    "            slices_ini = dperm.loc[:,u].unique()\n",
    "            median = dperm.loc[:,u].unique()[0]\n",
    "\n",
    "        # get center data\n",
    "        temp_list = []\n",
    "        col_names = []\n",
    "\n",
    "        tip_names = [s+'_c' for s in tip]\n",
    "        col_names = ['v'] + tip_names \n",
    "        if not probe=='ftir':\n",
    "            col_names = col_names + probe_settings[probe]['names'][h:]\n",
    "        col_name_ordered = ['v'] + tip_names\n",
    "\n",
    "        for col in [v] + probe_settings[probe]['names'][2:]:\n",
    "            temp_list.append(dperm.loc[dperm.loc[:,u] == median, col].reset_index(drop = True))\n",
    "\n",
    "        if slices_ini.size>=3:\n",
    "            for p in tip:\n",
    "                for k in slices_ini[[0,-1]]:    \n",
    "                    temp_list.append(dperm.loc[dperm.loc[:,u] == k,p].reset_index(drop = True))\n",
    "                col_names = col_names + [p + '_l', p + '_r']\n",
    "                col_name_ordered = col_name_ordered + [p + '_l', p + '_r']\n",
    "\n",
    "        df_temp_lines = pd.concat(temp_list, axis = 1, ignore_index=True)\n",
    "\n",
    "        df_temp_lines.columns =  col_names\n",
    "\n",
    "        # re order slices\n",
    "        if not probe=='ftir':\n",
    "            col_name_ordered = col_name_ordered + probe_settings[probe]['names'][h:]\n",
    "        df_temp_lines = df_temp_lines.loc[:, col_name_ordered]\n",
    "\n",
    "        # add extra information (in case needed for statistical analysis)\n",
    "        df_temp_lines['side'] = s[-1]['side']\n",
    "        df_temp_lines['code'] = s[-1]['sample_code']\n",
    "        df_temp_lines['family'] = s[-1]['sample_family']\n",
    "        df_temp_lines['tag'] = s[-1]['sample_tag']\n",
    "        df_temp_lines['sub_tag'] = s[-1]['subsample_tag']\n",
    "\n",
    "        df_temp_lines.to_csv(os.path.join(postprocesspath, fname + '_lines.csv'), index = False)\n",
    "        lines.append(df_temp_lines)\n",
    "\n",
    "# concat and save\n",
    "df_pool = pd.concat(pool, ignore_index=True)\n",
    "df_lines = pd.concat(lines, ignore_index=True, sort=False)\n",
    "\n",
    "df_pool.to_csv(os.path.join(datapath, '_postprocessed', probe + '_pool.csv'), index = False)\n",
    "df_lines.to_csv(os.path.join(datapath, '_postprocessed', probe + '_lines.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
