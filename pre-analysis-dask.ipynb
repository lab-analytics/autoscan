{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing with dask\n",
    "import os, sys, re, io, pathlib\n",
    "import pandas as pd\n",
    "import hiplot as hip\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "# packages needed to use dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import multiprocessing.popen_spawn_posix\n",
    "from distributed import Client, LocalCluster\n",
    "\n",
    "# limit memory to 1 GB\n",
    "# client = Client(n_workers=4, threads_per_worker=1, memory_limit=4e9)\n",
    "\n",
    "buffer = io.StringIO()\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# define the current path (notebooks in lab_utils)\n",
    "labutilspath = str(pathlib.Path(os.getcwd()).parents[1])\n",
    "sys.path.append(labutilspath)\n",
    "\n",
    "# import the autoscan routines\n",
    "from autoscan import autoscan\n",
    "\n",
    "pp = autoscan.basics(material_info = True)\n",
    "\n",
    "ftir_cols = pp.probe_settings['ftir']['col'][2:]\n",
    "tips_cols = list(itertools.chain(*[p['col'][2:] for _, p in pp.probe_settings.items()]))\n",
    "ftir_lambdas = pp.probe_settings['ftir']['lambdas']\n",
    "\n",
    "def pprint(msg, msg_title = '', msg_decorator = '#', len_decorator = 40):\n",
    "    nhead = len_decorator - len(msg_title) - 2\n",
    "    if nhead <= 0:\n",
    "        nhead = 1\n",
    "        nfoot = len(msg_title) + 4\n",
    "    else:\n",
    "        nfoot = len_decorator\n",
    "    \n",
    "    top_decorator = msg_decorator * (nhead // 2) \n",
    "    print(top_decorator + ' ' + msg_title  +  ' ' + top_decorator, \n",
    "          msg, nfoot * '#' + '\\n',\n",
    "          sep = '\\n')\n",
    "    return\n",
    "\n",
    "def dfinfo(df, header = 'info'):\n",
    "    with io.StringIO() as buffer:\n",
    "        df.info(buf = buffer)\n",
    "        pprint(buffer.getvalue(), msg_title = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(name = 'dask', n_workers = 11, threads_per_worker = 2)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = '/home/urlab/sandbox/data/characterization/autoscan/autoscan.h5'\n",
    "datapath = '/sandbox/data/autoscan/'\n",
    "datafile = os.path.join(datapath, 'autoscan.h5')\n",
    "savepath = datapath\n",
    "\n",
    "# load the data\n",
    "da = dd.read_hdf(datafile, '/data', chunksize = 10000)\n",
    "da = da.repartition(npartitions = 22, force = True)\n",
    "dn = da.iloc[:, -1760:].copy()\n",
    "ds = da.iloc[:, :8].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds.index.name = 'ix'\n",
    "ds = ds.assign(rho = 0.0)\n",
    "ds = ds.assign(\n",
    "    basetag = ds.tag.str.split('_', expand = True, n = 1)[0].values\n",
    ")\n",
    "\n",
    "for t in ds.basetag.unique():\n",
    "    ds['rho'] = ds['rho'].mask(ds['basetag'] == t, pp.get_material_density(t))\n",
    "\n",
    "ds = ds.compute()\n",
    "# ds = ds.set_index(['code', ds.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrepartitiond all metadata\n",
    "# desc = dd.read_hdf(datafile, '/description').compute()\n",
    "rho = ds.loc[:, 'rho'].values.reshape(ds.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_peak_to_lambda(x):\n",
    "    if np.logical_and(x != np.nan, type(x) == str):\n",
    "        out = ftir_lambdas[int(x.split('_')[1]) - 1]\n",
    "    else:\n",
    "        out = np.nan\n",
    "    return out\n",
    "\n",
    "def ftir_row_stats(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            l_mean = lambda df: df.loc[:, ftir_cols].mean(axis = 1),          \n",
    "            l_std = lambda df: df.loc[:, ftir_cols].std(axis = 1),\n",
    "            # l_median = lambda df: np.median(df.iloc[:, 2:1754], axis = 1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def ftir_extreme_locations(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    idx_max_peaks = df.loc[:, ftir_cols].idxmax(axis = 1)\n",
    "    idx_min_peaks = df.loc[:, ftir_cols].idxmin(axis = 1)\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            l_max_peak = idx_max_peaks.apply(lambda x: idx_peak_to_lambda(x), meta = pd.Series([], dtype=float, name='x')),\n",
    "            l_min_peak = idx_min_peaks.apply(lambda x: idx_peak_to_lambda(x), meta = pd.Series([], dtype=float, name='x'))        \n",
    "        )\n",
    "    \n",
    "    )\n",
    "    \n",
    "def clean_dataframe(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .where(df >= 0, np.nan)\n",
    "        .astype(np.float32)\n",
    "    )\n",
    "\n",
    "def enforce_limits(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    for k, p in pp.probe_settings.items():\n",
    "        v = p['col'][2:]\n",
    "        vmin, vmax = p['limits']\n",
    "        df[v] = df[v].where(((df[v] >= vmin) & (df[v] <= vmax)), np.nan)\n",
    "    return df\n",
    "\n",
    "def _direction_selection(direction):\n",
    "    if type(direction) != str:\n",
    "        direction = str(direction)\n",
    "    sel = ['vp' + direction + 'p2', 'vs' + direction + 'p2']\n",
    "    return sel\n",
    "    \n",
    "def calculate_shear(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            shear0  = df.loc[:, 'rho'] * df.loc[:, 'vp0p2'],\n",
    "            shear90 = df.loc[:, 'rho'] * df.loc[:, 'vp90p2']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _calculate_E(df: dd.DataFrame, direction = 0) -> dd.DataFrame:\n",
    "    sel = _direction_selection(direction)\n",
    "    E = 3.0 * df.loc[:, sel[0]] - 4.0 * df.loc[:, sel[1]]\n",
    "    E = df.loc[:, 'rho'] * df.loc[:, sel[1]] * E\n",
    "    E = np.divide(E, df.loc[:, sel[0]] - df.loc[:, sel[1]])\n",
    "    \n",
    "    return E\n",
    "\n",
    "def _calculate_k(df: dd.DataFrame, direction = 0) -> dd.DataFrame:\n",
    "    sel = _direction_selection(direction)\n",
    "    k = df.loc[:, 'rho'] *  (df.loc[:, sel[0]] - (4.0/3.0) * df.loc[:, sel[1]])\n",
    "    return k\n",
    "\n",
    "def _calculate_nu(df: dd.DataFrame, direction = 0) -> dd.DataFrame:\n",
    "    sel = _direction_selection(direction)\n",
    "    nu = np.divide(df.loc[:, sel[0]] - 2.0 * df.loc[:, sel[1]], 2.0 * (df.loc[:, sel[0]] - df.loc[:, sel[1]]))\n",
    "    return nu\n",
    "\n",
    "def _calculate_l(df: dd.DataFrame, direction = 0) -> dd.DataFrame:\n",
    "    sel = _direction_selection(direction)\n",
    "    l = df.loc[:, 'rho'] * (df.loc[:, sel[0]] - 2.0 * df.loc[:, sel[1]])\n",
    "    return l\n",
    "\n",
    "# def calculate_E0(df: dd.DataFrame) -> dd.DataFrame:\n",
    "#     return(\n",
    "#         df\n",
    "#         .assign(\n",
    "#             E0 = calculate_E(df, direction = 0)\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# def calculate_E90(df: dd.DataFrame) -> dd.DataFrame:\n",
    "#     return(\n",
    "#         df\n",
    "#         .assign(\n",
    "#             E90 = _calculate_E(df, direction = 90)\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "def calculate_young(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return(\n",
    "        df\n",
    "        .assign(\n",
    "            E0  = _calculate_E(df, direction = 0),\n",
    "            E90 = _calculate_E(df, direction = 90)\n",
    "        )\n",
    "    )\n",
    "def calculate_bulk(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return(\n",
    "        df\n",
    "        .assign(\n",
    "            k0  = _calculate_k(df, direction = 0),\n",
    "            k90 = _calculate_k(df, direction = 90) \n",
    "            \n",
    "        )\n",
    "    )\n",
    "\n",
    "def calculate_poisson(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return(\n",
    "        df\n",
    "        .assign(\n",
    "            nu0  = _calculate_nu(df, direction = 0),\n",
    "            nu90 = _calculate_nu(df, direction = 90) \n",
    "            \n",
    "        )\n",
    "    )\n",
    "\n",
    "def calculate_lame(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    return(\n",
    "        df\n",
    "        .assign(\n",
    "            la0  = _calculate_nu(df, direction = 0),\n",
    "            la90 = _calculate_nu(df, direction = 90) \n",
    "            \n",
    "        )\n",
    "    )\n",
    "\n",
    "# def lame(vel, rho = 1):\n",
    "#     v2 = np.power(vel, 2)\n",
    "#     l  = rho * (v2[:, 0] - 2 * v2[:, 1])\n",
    "#     return \n",
    "\n",
    "# def bulk(vel, rho = 1):\n",
    "#     v2 = np.power(vel, 2)\n",
    "#     k  = rho * (v2[:, 0] - (4.0/3.0) * v2[:, 1])\n",
    "#     return k\n",
    "\n",
    "# def nu(vel, rho = 1):\n",
    "#     v2 = np.power(vel, 2)\n",
    "#     nu = np.divide(v2[:, 0] - 2 * v2[:, 1], 2 * (v2[:, 0] - v2[:, 1]))\n",
    "#     return nu\n",
    "\n",
    "def compute_final_dataframe(df: dd.DataFrame, workers = 20) -> pd.DataFrame:\n",
    "    \"\"\"Execute dask task graph and compute final results\"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .compute(num_workers = 6)\n",
    "    )\n",
    "\n",
    "def hip_visualize(df, pcols = None, index = ['family', 'code']):\n",
    "    dp = df.reset_index().loc[:, np.append(index, pcols)]\n",
    "    s = hip.Experiment.from_dataframe(dp)\n",
    "    s.colormap = 'interpolateViridis'\n",
    "    s.display()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcols = ['vp0', 'vp90', 'vs0', 'vs90']\n",
    "vcols2 = [v + 'p2' for v in vcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = dn.join(ds['rho'].reset_index(drop = True))\n",
    "dn[vcols2] = dn.loc[:, vcols].pow(2)\n",
    "dn = clean_dataframe(dn)\n",
    "dn = enforce_limits(dn)\n",
    "dn = ftir_row_stats(dn)\n",
    "dn = ftir_extreme_locations(dn)\n",
    "dn = dn.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = calculate_shear(dn)\n",
    "dn = calculate_bulk(dn)\n",
    "dn = calculate_poisson(dn)\n",
    "dn = calculate_lame(dn)\n",
    "dn = calculate_young(dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dn.compute()\n",
    "df.index.name = 'ix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_max_peaks = df.loc[:, ftir_cols].idxmax(axis = 1)\n",
    "# idx_min_peaks = df.loc[:, ftir_cols].idxmin(axis = 1)\n",
    "# ixmax = idx_max_peaks.apply(lambda x: idx_peak_to_lambda(x))\n",
    "# ixmin = idx_min_peaks.apply(lambda x: idx_peak_to_lambda(x))\n",
    "# mx = df.loc[:, 'l_max_peak'] != ixmax\n",
    "# mn = df.loc[:, 'l_min_peak'] != ixmin\n",
    "\n",
    "# # print(mx, mn)\n",
    "# # df.loc[:, 'l_min_peak'] = idx_min_peaks.apply(lambda x: idx_peak_to_lambda(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v = dn.loc[:, 'rho'] *  dn.loc[:, 'x']\n",
    "v.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = hip_visualize(df.dropna(subset = ['perm', 'vp0', 'vs0', 'e_star', 'l_max_peak']), \n",
    "#                   pcols = ['l_max_peak', 'l_min_peak', 'perm', 'vp0', 'vs0', 'e_star'], \n",
    "#                   index = ['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-sigma",
   "metadata": {},
   "source": [
    "# data cleaning with klib\n",
    "1. pre-clean the dataset\n",
    " - remove duplicated rows\n",
    " - enforce correct dtypes \n",
    " - reduce memory overhead\n",
    " - do not remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import klib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the record print the information of the original dataframe\n",
    "dfinfo(df, 'raw data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pre-clean, do not remove missing values\n",
    "# df = klib.data_cleaning(df, drop_threshold_rows = 1.0, clean_col_names = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_numerical = ['x', 'y'] + tips_cols + ['l_mean', 'l_std', 'l_max_peak', 'l_min_peak']\n",
    "col_categorical = ['family', 'code', 'tag', 'subtag', 'instance', 'experiment', 'side', 'm']\n",
    "df.loc[:, col_numerical] = df.loc[:, col_numerical].astype(np.float32)\n",
    "# df.loc[:, col_categorical] = df.loc[:, col_categorical].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the information of the cleaned dataframe\n",
    "dfinfo(df, 'raw data cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-information",
   "metadata": {},
   "source": [
    "## fix values and correct information\n",
    "1. set nan to measurements where all values are the same (ftir)\n",
    "2. set the correct family and code for eur samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = df.loc[:, ftir_cols].apply(lambda x: len(np.unique(x)), axis = 1) == 1\n",
    "df.loc[ix, ftir_cols] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-advisory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.tag.str.contains('eur'), 'family'] = 'shale'\n",
    "df.loc[df.tag.str.contains('eur'), 'code'] = 'sh'\n",
    "df.loc[:, col_categorical] = df.loc[:, col_categorical].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, col_numerical].to_hdf(os.path.join('/sandbox/data/', 'autoscan_corrected.h5'), key = 'data', format = 'table', mode = 'w')\n",
    "df.loc[:, col_categorical].to_hdf(os.path.join('/sandbox/data/', 'autoscan_corrected.h5'), key = 'desc', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_ftir = df.loc[ix, :].set_index(col_categorical[:-1]).index.unique()\n",
    "pd.DataFrame.from_records(repeat_ftir.to_numpy(), columns = col_categorical[:-1]).to_csv(os.path.join(datapath, 'ftir_repeat.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-office",
   "metadata": {},
   "source": [
    "# visualization\n",
    "1. hip-plot (again) but with corrected data\n",
    "2. distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-license",
   "metadata": {},
   "source": [
    "## hip\n",
    "### without `e_star`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hip_visualize(df.query(\"instance == 'before'\").dropna(subset = ['perm', 'vp0', 'vs0', 'l_max_peak']), \n",
    "                  pcols = ['l_max_peak', 'l_min_peak', 'perm', 'vp0', 'vs0'], \n",
    "                  index = ['code'])\n",
    "\n",
    "s.to_html(os.path.join(savepath, 'hip_before_woestar.html'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hip_visualize(df.query(\"instance == 'after'\").dropna(subset = ['perm', 'vp0', 'vs0', 'l_max_peak']), \n",
    "                  pcols = ['l_max_peak', 'l_min_peak', 'perm', 'vp0', 'vs0'], \n",
    "                  index = ['code'])\n",
    "\n",
    "s.to_html(os.path.join(savepath, 'hip_before_westar.html'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-market",
   "metadata": {},
   "source": [
    "### with `e_star`\n",
    "the number of samples with impulse hammer measurements are 1/4th of the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hip_visualize(df.query(\"instance == 'before'\").dropna(subset = ['perm', 'vp0', 'vs0', 'e_star', 'l_max_peak']), \n",
    "                  pcols = ['l_max_peak', 'l_min_peak', 'perm', 'vp0', 'vs0', 'e_star'], \n",
    "                  index = ['code'])\n",
    "\n",
    "s.to_html(os.path.join(savepath, 'hip_after_woestar.html'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hip_visualize(df.query(\"instance == 'after'\").dropna(subset = ['perm', 'vp0', 'vs0', 'e_star', 'l_max_peak']), \n",
    "                  pcols = ['l_max_peak', 'l_min_peak', 'perm', 'vp0', 'vs0', 'e_star'], \n",
    "                  index = ['code'])\n",
    "\n",
    "s.to_html(os.path.join(savepath, 'hip_after_westar.html'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hip_visualize(df.query(\"instance == 'before'\").dropna(subset = ['perm', 'vp0', 'vs0']), \n",
    "                  pcols = ['perm', 'vp0', 'vs0'], \n",
    "                  index = ['code'])\n",
    "\n",
    "s.to_html(os.path.join(savepath, 'hip_before_permvel.html'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hip_visualize(df.query(\"instance == 'after'\").dropna(subset = ['perm', 'vp0', 'vs0']), \n",
    "                  pcols = ['perm', 'vp0', 'vs0'], \n",
    "                  index = ['code'])\n",
    "\n",
    "s.to_html(os.path.join(savepath, 'hip_after_permvel.html'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df.query(\"instance == 'before'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_perm = df_before.perm.isna() == False\n",
    "df_perm_before = df_before.loc[ix_perm, ['family', 'code', 'perm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "sns.stripplot(y = 'perm', x = 'family', hue = 'code', data = df_perm_before, palette = 'viridis', ax = ax)\n",
    "plt.yscale('log')\n",
    "sns.set_style('darkgrid')\n",
    "plt.title('permeability before');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "sns.violinplot(y = 'perm', x = 'code', hue = 'family', data = df_perm_before, palette = 'viridis', ax = ax)\n",
    "sns.set_style('darkgrid')\n",
    "plt.title('permeability before');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perm_before_clipped = df_perm_before.copy()\n",
    "df_perm_before_clipped.loc[:, 'perm'] = df_perm_before_clipped.perm.clip(lower = 0, upper = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "sns.violinplot(y = 'perm', x = 'code', hue = 'family', data = df_perm_before_clipped, palette = 'viridis', ax = ax)\n",
    "sns.set_style('darkgrid')\n",
    "plt.title('permeability before');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "sns.boxplot(y = 'perm', x = 'code', hue = 'family', data = df_perm_before_clipped, palette = 'viridis', ax = ax)\n",
    "sns.set_style('darkgrid')\n",
    "plt.title('permeability before');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "sns.kdeplot(x = 'perm',  hue = 'code', data = df_perm_before_clipped, \n",
    "            palette = 'viridis', shade = 'fill', ax = ax)\n",
    "sns.set_style('darkgrid')\n",
    "plt.title('permeability before');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ds.tag.str.split('_', expand = True)#.apply(lambda x: pp.get_material_density(x))\n",
    "tags[1] = 0.0\n",
    "unique_tags = tags[0].unique()\n",
    "# tags.set_index([0, tags.index], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:numerics] *",
   "language": "python",
   "name": "conda-env-numerics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
